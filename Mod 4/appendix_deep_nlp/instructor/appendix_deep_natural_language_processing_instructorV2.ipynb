{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mod 4 Appendix: Deep NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bonus Office Hours S.G.\n",
    "- 11/18/20\n",
    "- online-ds-pt-041320"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You are not allowed to use any propriety functions from this notebook in your projects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discuss word Embeddings and their advantages\n",
    "- Training Word2Vec models\n",
    "- Using pretrained word embeddings\n",
    "\n",
    "\n",
    "- Create a Classification Model for true-trump (\"Twitter for Android\") vs trump-staffer(\"Twitter for iPhone - from period of time when android was still in use)\n",
    "\n",
    "    - Use lesson's W2Vec class in Sci-kit learn models\n",
    "    - Use LSTMs\n",
    "    - Use RNN/GRUs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Compare:\n",
    "    1.  Mean embeddings vs count/tfidf data with scikit learn.\n",
    "    \n",
    "\n",
    "<!-- ## References\n",
    "\n",
    "- My Work-in-Progress Capstone v2.0 Notebook:\n",
    "    - [GitHub Notebook Link](https://github.com/jirvingphd/capstone-project-using-trumps-tweets-to-predict-stock-market/blob/WIP/Capstone%20Restarted%2010-2020.ipynb) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP & Word Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Natural Language Processing_**, or **_NLP_**, is the study of how computers can interact with humans through the use of human language.  Although this is a field that is quite important to Data Scientists, it does not belong to Data Science alone.  NLP has been around for quite a while, and sits at the intersection of *Computer Science*, *Artificial Intelligence*, *Linguistics*, and *Information Theory*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-word-embeddings-online-ds-ft-100719/master/images/embeddings.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert words into a vector space\n",
    "    + Mathematical object\n",
    "- It's all about closeness\n",
    "    + Distributional Hypothesis: https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-word-embeddings-online-ds-ft-100719/master/images/vectors.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kaggle Tutorial:  https://www.kaggle.com/learn/embeddings\n",
    "- Google Embedding Crash Course: https://developers.google.com/machine-learning/crash-course/embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-using-word2vec-online-ds-ft-100719/master/images/training_data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the MLP to find the best weights (context) to map word-to-word\n",
    "- But since words close to another usually contain context, we're _really_ teaching it context in those weights\n",
    "- Gut check: similar contexted words can be exchanged\n",
    "    + EX: \"A fluffy **dog** is a great pet\" <--> \"A fluffy **cat** is a great pet\"\n",
    "\n",
    "- By training a text-generation model, we wind up with a lookup table where each word has its own vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-using-word2vec-online-ds-ft-100719/master/images/new_skip_gram_net_arch.png\">\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-using-word2vec-online-ds-ft-100719/master/images/new_word2vec_weight_matrix_lookup_table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word will have a vector of contexts: the embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Word Embeddings with Trump's Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:04.412459Z",
     "start_time": "2020-11-18T23:13:58.570967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fsds v0.2.27 loaded.  Read the docs: https://fs-ds.readthedocs.io/en/latest/ \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122\" ><caption>Loaded Packages and Handles</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >Handle</th>        <th class=\"col_heading level0 col1\" >Package</th>        <th class=\"col_heading level0 col2\" >Description</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row0_col0\" class=\"data row0 col0\" >dp</td>\n",
       "                        <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row0_col1\" class=\"data row0 col1\" >IPython.display</td>\n",
       "                        <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row0_col2\" class=\"data row0 col2\" >Display modules with helpful display and clearing commands.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row1_col0\" class=\"data row1 col0\" >fs</td>\n",
       "                        <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row1_col1\" class=\"data row1 col1\" >fsds</td>\n",
       "                        <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row1_col2\" class=\"data row1 col2\" >Custom data science bootcamp student package</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row2_col0\" class=\"data row2 col0\" >mpl</td>\n",
       "                        <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row2_col1\" class=\"data row2 col1\" >matplotlib</td>\n",
       "                        <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row2_col2\" class=\"data row2 col2\" >Matplotlib's base OOP module with formatting artists</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row3_col0\" class=\"data row3 col0\" >plt</td>\n",
       "                        <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row3_col1\" class=\"data row3 col1\" >matplotlib.pyplot</td>\n",
       "                        <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row3_col2\" class=\"data row3 col2\" >Matplotlib's matlab-like plotting module</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row4_col0\" class=\"data row4 col0\" >np</td>\n",
       "                        <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row4_col1\" class=\"data row4 col1\" >numpy</td>\n",
       "                        <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row4_col2\" class=\"data row4 col2\" >scientific computing with Python</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row5_col0\" class=\"data row5 col0\" >pd</td>\n",
       "                        <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row5_col1\" class=\"data row5 col1\" >pandas</td>\n",
       "                        <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row5_col2\" class=\"data row5 col2\" >High performance data structures and tools</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row6_col0\" class=\"data row6 col0\" >sns</td>\n",
       "                        <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row6_col1\" class=\"data row6 col1\" >seaborn</td>\n",
       "                        <td id=\"T_bfd4c136_29f3_11eb_bca0_acde48001122row6_col2\" class=\"data row6 col2\" >High-level data visualization library based on matplotlib</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa9d01480f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Pandas .iplot() method activated.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U fsds\n",
    "from fsds.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:06.324129Z",
     "start_time": "2020-11-18T23:14:04.414386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-12-01 14:37:57</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>My thoughts and prayers are with those affecte...</td>\n",
       "      <td>12077</td>\n",
       "      <td>65724</td>\n",
       "      <td>False</td>\n",
       "      <td>804333718999539712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01 14:38:09</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Getting ready to leave for the Great State of ...</td>\n",
       "      <td>9834</td>\n",
       "      <td>57249</td>\n",
       "      <td>False</td>\n",
       "      <td>804333771021570048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01 22:52:10</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Heading to U.S. Bank Arena in Cincinnati Ohio ...</td>\n",
       "      <td>5564</td>\n",
       "      <td>31256</td>\n",
       "      <td>False</td>\n",
       "      <td>804458095569158144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-02 02:45:18</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Thank you Ohio! Together we made history – and...</td>\n",
       "      <td>17283</td>\n",
       "      <td>72196</td>\n",
       "      <td>False</td>\n",
       "      <td>804516764562374656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-03 00:44:20</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>The President of Taiwan CALLED ME today to wis...</td>\n",
       "      <td>24700</td>\n",
       "      <td>111106</td>\n",
       "      <td>False</td>\n",
       "      <td>804848711599882240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:17:43</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @SenJohnKennedy: I think Speaker Pelosi is ...</td>\n",
       "      <td>8893</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1212181071988703232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:18:47</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @DanScavino: https://t.co/CJRPySkF1Z</td>\n",
       "      <td>10796</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1212181341078458369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:22:28</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Our fantastic First Lady! https://t.co/6iswto4WDI</td>\n",
       "      <td>27567</td>\n",
       "      <td>132633</td>\n",
       "      <td>False</td>\n",
       "      <td>1212182267113680896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:30:35</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>HAPPY NEW YEAR!</td>\n",
       "      <td>85409</td>\n",
       "      <td>576045</td>\n",
       "      <td>False</td>\n",
       "      <td>1212184310389850119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 03:12:07</th>\n",
       "      <td>Twitter Media Studio</td>\n",
       "      <td>https://t.co/EVAEYD1AgV</td>\n",
       "      <td>25016</td>\n",
       "      <td>108830</td>\n",
       "      <td>False</td>\n",
       "      <td>1212209862094012416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14066 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   source  \\\n",
       "created_at                                  \n",
       "2016-12-01 14:37:57    Twitter for iPhone   \n",
       "2016-12-01 14:38:09   Twitter for Android   \n",
       "2016-12-01 22:52:10    Twitter for iPhone   \n",
       "2016-12-02 02:45:18    Twitter for iPhone   \n",
       "2016-12-03 00:44:20   Twitter for Android   \n",
       "...                                   ...   \n",
       "2020-01-01 01:17:43    Twitter for iPhone   \n",
       "2020-01-01 01:18:47    Twitter for iPhone   \n",
       "2020-01-01 01:22:28    Twitter for iPhone   \n",
       "2020-01-01 01:30:35    Twitter for iPhone   \n",
       "2020-01-01 03:12:07  Twitter Media Studio   \n",
       "\n",
       "                                                                  text  \\\n",
       "created_at                                                               \n",
       "2016-12-01 14:37:57  My thoughts and prayers are with those affecte...   \n",
       "2016-12-01 14:38:09  Getting ready to leave for the Great State of ...   \n",
       "2016-12-01 22:52:10  Heading to U.S. Bank Arena in Cincinnati Ohio ...   \n",
       "2016-12-02 02:45:18  Thank you Ohio! Together we made history – and...   \n",
       "2016-12-03 00:44:20  The President of Taiwan CALLED ME today to wis...   \n",
       "...                                                                ...   \n",
       "2020-01-01 01:17:43  RT @SenJohnKennedy: I think Speaker Pelosi is ...   \n",
       "2020-01-01 01:18:47            RT @DanScavino: https://t.co/CJRPySkF1Z   \n",
       "2020-01-01 01:22:28  Our fantastic First Lady! https://t.co/6iswto4WDI   \n",
       "2020-01-01 01:30:35                                    HAPPY NEW YEAR!   \n",
       "2020-01-01 03:12:07                            https://t.co/EVAEYD1AgV   \n",
       "\n",
       "                     retweet_count  favorite_count is_retweet  \\\n",
       "created_at                                                      \n",
       "2016-12-01 14:37:57          12077           65724      False   \n",
       "2016-12-01 14:38:09           9834           57249      False   \n",
       "2016-12-01 22:52:10           5564           31256      False   \n",
       "2016-12-02 02:45:18          17283           72196      False   \n",
       "2016-12-03 00:44:20          24700          111106      False   \n",
       "...                            ...             ...        ...   \n",
       "2020-01-01 01:17:43           8893               0       True   \n",
       "2020-01-01 01:18:47          10796               0       True   \n",
       "2020-01-01 01:22:28          27567          132633      False   \n",
       "2020-01-01 01:30:35          85409          576045      False   \n",
       "2020-01-01 03:12:07          25016          108830      False   \n",
       "\n",
       "                                  id_str  \n",
       "created_at                                \n",
       "2016-12-01 14:37:57   804333718999539712  \n",
       "2016-12-01 14:38:09   804333771021570048  \n",
       "2016-12-01 22:52:10   804458095569158144  \n",
       "2016-12-02 02:45:18   804516764562374656  \n",
       "2016-12-03 00:44:20   804848711599882240  \n",
       "...                                  ...  \n",
       "2020-01-01 01:17:43  1212181071988703232  \n",
       "2020-01-01 01:18:47  1212181341078458369  \n",
       "2020-01-01 01:22:28  1212182267113680896  \n",
       "2020-01-01 01:30:35  1212184310389850119  \n",
       "2020-01-01 03:12:07  1212209862094012416  \n",
       "\n",
       "[14066 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = fs.datasets.load_nlp_finding_trump(read_csv_kwds={'parse_dates':['created_at'],\n",
    "                                                      'index_col':'created_at'})\n",
    "df.sort_index(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:06.341472Z",
     "start_time": "2020-11-18T23:14:06.326180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-12-01 14:38:09</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Getting ready to leave for the Great State of ...</td>\n",
       "      <td>9834</td>\n",
       "      <td>57249</td>\n",
       "      <td>False</td>\n",
       "      <td>804333771021570048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01 22:52:10</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Heading to U.S. Bank Arena in Cincinnati Ohio ...</td>\n",
       "      <td>5564</td>\n",
       "      <td>31256</td>\n",
       "      <td>False</td>\n",
       "      <td>804458095569158144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-02 02:45:18</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Thank you Ohio! Together we made history – and...</td>\n",
       "      <td>17283</td>\n",
       "      <td>72196</td>\n",
       "      <td>False</td>\n",
       "      <td>804516764562374656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-03 00:44:20</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>The President of Taiwan CALLED ME today to wis...</td>\n",
       "      <td>24700</td>\n",
       "      <td>111106</td>\n",
       "      <td>False</td>\n",
       "      <td>804848711599882240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-03 01:41:30</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Interesting how the U.S. sells Taiwan billions...</td>\n",
       "      <td>38805</td>\n",
       "      <td>122905</td>\n",
       "      <td>False</td>\n",
       "      <td>804863098138005504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-24 17:03:46</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Today I was pleased to announce the official a...</td>\n",
       "      <td>12933</td>\n",
       "      <td>66692</td>\n",
       "      <td>False</td>\n",
       "      <td>845320243614547968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-24 17:59:42</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Today I was thrilled to announce a commitment ...</td>\n",
       "      <td>20212</td>\n",
       "      <td>89339</td>\n",
       "      <td>False</td>\n",
       "      <td>845334323045765121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-25 13:29:17</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Happy #MedalOfHonorDay to our heroes! ➡️https:...</td>\n",
       "      <td>14139</td>\n",
       "      <td>68302</td>\n",
       "      <td>False</td>\n",
       "      <td>845628655493677056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-25 14:37:52</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>ObamaCare will explode and we will all get tog...</td>\n",
       "      <td>22518</td>\n",
       "      <td>104321</td>\n",
       "      <td>False</td>\n",
       "      <td>845645916732358656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-25 14:41:14</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Watch @JudgeJeanine on @FoxNews tonight at 9:0...</td>\n",
       "      <td>10116</td>\n",
       "      <td>51247</td>\n",
       "      <td>False</td>\n",
       "      <td>845646761704243200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  source  \\\n",
       "created_at                                 \n",
       "2016-12-01 14:38:09  Twitter for Android   \n",
       "2016-12-01 22:52:10   Twitter for iPhone   \n",
       "2016-12-02 02:45:18   Twitter for iPhone   \n",
       "2016-12-03 00:44:20  Twitter for Android   \n",
       "2016-12-03 01:41:30  Twitter for Android   \n",
       "...                                  ...   \n",
       "2017-03-24 17:03:46   Twitter for iPhone   \n",
       "2017-03-24 17:59:42   Twitter for iPhone   \n",
       "2017-03-25 13:29:17   Twitter for iPhone   \n",
       "2017-03-25 14:37:52  Twitter for Android   \n",
       "2017-03-25 14:41:14  Twitter for Android   \n",
       "\n",
       "                                                                  text  \\\n",
       "created_at                                                               \n",
       "2016-12-01 14:38:09  Getting ready to leave for the Great State of ...   \n",
       "2016-12-01 22:52:10  Heading to U.S. Bank Arena in Cincinnati Ohio ...   \n",
       "2016-12-02 02:45:18  Thank you Ohio! Together we made history – and...   \n",
       "2016-12-03 00:44:20  The President of Taiwan CALLED ME today to wis...   \n",
       "2016-12-03 01:41:30  Interesting how the U.S. sells Taiwan billions...   \n",
       "...                                                                ...   \n",
       "2017-03-24 17:03:46  Today I was pleased to announce the official a...   \n",
       "2017-03-24 17:59:42  Today I was thrilled to announce a commitment ...   \n",
       "2017-03-25 13:29:17  Happy #MedalOfHonorDay to our heroes! ➡️https:...   \n",
       "2017-03-25 14:37:52  ObamaCare will explode and we will all get tog...   \n",
       "2017-03-25 14:41:14  Watch @JudgeJeanine on @FoxNews tonight at 9:0...   \n",
       "\n",
       "                     retweet_count  favorite_count is_retweet  \\\n",
       "created_at                                                      \n",
       "2016-12-01 14:38:09           9834           57249      False   \n",
       "2016-12-01 22:52:10           5564           31256      False   \n",
       "2016-12-02 02:45:18          17283           72196      False   \n",
       "2016-12-03 00:44:20          24700          111106      False   \n",
       "2016-12-03 01:41:30          38805          122905      False   \n",
       "...                            ...             ...        ...   \n",
       "2017-03-24 17:03:46          12933           66692      False   \n",
       "2017-03-24 17:59:42          20212           89339      False   \n",
       "2017-03-25 13:29:17          14139           68302      False   \n",
       "2017-03-25 14:37:52          22518          104321      False   \n",
       "2017-03-25 14:41:14          10116           51247      False   \n",
       "\n",
       "                                 id_str  \n",
       "created_at                               \n",
       "2016-12-01 14:38:09  804333771021570048  \n",
       "2016-12-01 22:52:10  804458095569158144  \n",
       "2016-12-02 02:45:18  804516764562374656  \n",
       "2016-12-03 00:44:20  804848711599882240  \n",
       "2016-12-03 01:41:30  804863098138005504  \n",
       "...                                 ...  \n",
       "2017-03-24 17:03:46  845320243614547968  \n",
       "2017-03-24 17:59:42  845334323045765121  \n",
       "2017-03-25 13:29:17  845628655493677056  \n",
       "2017-03-25 14:37:52  845645916732358656  \n",
       "2017-03-25 14:41:14  845646761704243200  \n",
       "\n",
       "[588 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Getting time period with android tweets\n",
    "droid_ts = df[df['source'] == 'Twitter for Android'].index\n",
    "find_trump = df.loc[droid_ts[0]:droid_ts[-1]].copy()\n",
    "\n",
    "## Getting only original-text (not retweets)\n",
    "find_trump = find_trump[find_trump['is_retweet']==False]\n",
    "find_trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:06.351362Z",
     "start_time": "2020-11-18T23:14:06.343381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trump        0.619048\n",
       "Not Trump    0.380952\n",
       "Name: is_trump, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_trump(x):\n",
    "    return 'Trump' if x =='Twitter for Android' else 'Not Trump'\n",
    "find_trump['is_trump'] = find_trump['source'].map(is_trump)\n",
    "find_trump['is_trump'].value_counts(dropna=False,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:06.371147Z",
     "start_time": "2020-11-18T23:14:06.352804Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "      <th>is_trump</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-12-01 14:38:09</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Getting ready to leave for the Great State of ...</td>\n",
       "      <td>9834</td>\n",
       "      <td>57249</td>\n",
       "      <td>False</td>\n",
       "      <td>804333771021570048</td>\n",
       "      <td>Trump</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01 22:52:10</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Heading to U.S. Bank Arena in Cincinnati Ohio ...</td>\n",
       "      <td>5564</td>\n",
       "      <td>31256</td>\n",
       "      <td>False</td>\n",
       "      <td>804458095569158144</td>\n",
       "      <td>Not Trump</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-02 02:45:18</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Thank you Ohio! Together we made history – and...</td>\n",
       "      <td>17283</td>\n",
       "      <td>72196</td>\n",
       "      <td>False</td>\n",
       "      <td>804516764562374656</td>\n",
       "      <td>Not Trump</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-03 00:44:20</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>The President of Taiwan CALLED ME today to wis...</td>\n",
       "      <td>24700</td>\n",
       "      <td>111106</td>\n",
       "      <td>False</td>\n",
       "      <td>804848711599882240</td>\n",
       "      <td>Trump</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-03 01:41:30</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Interesting how the U.S. sells Taiwan billions...</td>\n",
       "      <td>38805</td>\n",
       "      <td>122905</td>\n",
       "      <td>False</td>\n",
       "      <td>804863098138005504</td>\n",
       "      <td>Trump</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-24 17:03:46</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Today I was pleased to announce the official a...</td>\n",
       "      <td>12933</td>\n",
       "      <td>66692</td>\n",
       "      <td>False</td>\n",
       "      <td>845320243614547968</td>\n",
       "      <td>Not Trump</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-24 17:59:42</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Today I was thrilled to announce a commitment ...</td>\n",
       "      <td>20212</td>\n",
       "      <td>89339</td>\n",
       "      <td>False</td>\n",
       "      <td>845334323045765121</td>\n",
       "      <td>Not Trump</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-25 13:29:17</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Happy #MedalOfHonorDay to our heroes! ➡️https:...</td>\n",
       "      <td>14139</td>\n",
       "      <td>68302</td>\n",
       "      <td>False</td>\n",
       "      <td>845628655493677056</td>\n",
       "      <td>Not Trump</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-25 14:37:52</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>ObamaCare will explode and we will all get tog...</td>\n",
       "      <td>22518</td>\n",
       "      <td>104321</td>\n",
       "      <td>False</td>\n",
       "      <td>845645916732358656</td>\n",
       "      <td>Trump</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-25 14:41:14</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Watch @JudgeJeanine on @FoxNews tonight at 9:0...</td>\n",
       "      <td>10116</td>\n",
       "      <td>51247</td>\n",
       "      <td>False</td>\n",
       "      <td>845646761704243200</td>\n",
       "      <td>Trump</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  source  \\\n",
       "created_at                                 \n",
       "2016-12-01 14:38:09  Twitter for Android   \n",
       "2016-12-01 22:52:10   Twitter for iPhone   \n",
       "2016-12-02 02:45:18   Twitter for iPhone   \n",
       "2016-12-03 00:44:20  Twitter for Android   \n",
       "2016-12-03 01:41:30  Twitter for Android   \n",
       "...                                  ...   \n",
       "2017-03-24 17:03:46   Twitter for iPhone   \n",
       "2017-03-24 17:59:42   Twitter for iPhone   \n",
       "2017-03-25 13:29:17   Twitter for iPhone   \n",
       "2017-03-25 14:37:52  Twitter for Android   \n",
       "2017-03-25 14:41:14  Twitter for Android   \n",
       "\n",
       "                                                                  text  \\\n",
       "created_at                                                               \n",
       "2016-12-01 14:38:09  Getting ready to leave for the Great State of ...   \n",
       "2016-12-01 22:52:10  Heading to U.S. Bank Arena in Cincinnati Ohio ...   \n",
       "2016-12-02 02:45:18  Thank you Ohio! Together we made history – and...   \n",
       "2016-12-03 00:44:20  The President of Taiwan CALLED ME today to wis...   \n",
       "2016-12-03 01:41:30  Interesting how the U.S. sells Taiwan billions...   \n",
       "...                                                                ...   \n",
       "2017-03-24 17:03:46  Today I was pleased to announce the official a...   \n",
       "2017-03-24 17:59:42  Today I was thrilled to announce a commitment ...   \n",
       "2017-03-25 13:29:17  Happy #MedalOfHonorDay to our heroes! ➡️https:...   \n",
       "2017-03-25 14:37:52  ObamaCare will explode and we will all get tog...   \n",
       "2017-03-25 14:41:14  Watch @JudgeJeanine on @FoxNews tonight at 9:0...   \n",
       "\n",
       "                     retweet_count  favorite_count is_retweet  \\\n",
       "created_at                                                      \n",
       "2016-12-01 14:38:09           9834           57249      False   \n",
       "2016-12-01 22:52:10           5564           31256      False   \n",
       "2016-12-02 02:45:18          17283           72196      False   \n",
       "2016-12-03 00:44:20          24700          111106      False   \n",
       "2016-12-03 01:41:30          38805          122905      False   \n",
       "...                            ...             ...        ...   \n",
       "2017-03-24 17:03:46          12933           66692      False   \n",
       "2017-03-24 17:59:42          20212           89339      False   \n",
       "2017-03-25 13:29:17          14139           68302      False   \n",
       "2017-03-25 14:37:52          22518          104321      False   \n",
       "2017-03-25 14:41:14          10116           51247      False   \n",
       "\n",
       "                                 id_str   is_trump  target  \n",
       "created_at                                                  \n",
       "2016-12-01 14:38:09  804333771021570048      Trump       1  \n",
       "2016-12-01 22:52:10  804458095569158144  Not Trump       0  \n",
       "2016-12-02 02:45:18  804516764562374656  Not Trump       0  \n",
       "2016-12-03 00:44:20  804848711599882240      Trump       1  \n",
       "2016-12-03 01:41:30  804863098138005504      Trump       1  \n",
       "...                                 ...        ...     ...  \n",
       "2017-03-24 17:03:46  845320243614547968  Not Trump       0  \n",
       "2017-03-24 17:59:42  845334323045765121  Not Trump       0  \n",
       "2017-03-25 13:29:17  845628655493677056  Not Trump       0  \n",
       "2017-03-25 14:37:52  845645916732358656      Trump       1  \n",
       "2017-03-25 14:41:14  845646761704243200      Trump       1  \n",
       "\n",
       "[588 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_map = {'Not Trump':0,'Trump':1}\n",
    "\n",
    "find_trump['target'] = find_trump['is_trump'].map(target_map)\n",
    "find_trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:06.384618Z",
     "start_time": "2020-11-18T23:14:06.372817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_trump</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-12-01 14:38:09</th>\n",
       "      <td>Getting ready to leave for the Great State of ...</td>\n",
       "      <td>Trump</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01 22:52:10</th>\n",
       "      <td>Heading to U.S. Bank Arena in Cincinnati Ohio ...</td>\n",
       "      <td>Not Trump</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-02 02:45:18</th>\n",
       "      <td>Thank you Ohio! Together we made history – and...</td>\n",
       "      <td>Not Trump</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-03 00:44:20</th>\n",
       "      <td>The President of Taiwan CALLED ME today to wis...</td>\n",
       "      <td>Trump</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-03 01:41:30</th>\n",
       "      <td>Interesting how the U.S. sells Taiwan billions...</td>\n",
       "      <td>Trump</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-24 17:03:46</th>\n",
       "      <td>Today I was pleased to announce the official a...</td>\n",
       "      <td>Not Trump</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-24 17:59:42</th>\n",
       "      <td>Today I was thrilled to announce a commitment ...</td>\n",
       "      <td>Not Trump</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-25 13:29:17</th>\n",
       "      <td>Happy #MedalOfHonorDay to our heroes! ➡️https:...</td>\n",
       "      <td>Not Trump</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-25 14:37:52</th>\n",
       "      <td>ObamaCare will explode and we will all get tog...</td>\n",
       "      <td>Trump</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-25 14:41:14</th>\n",
       "      <td>Watch @JudgeJeanine on @FoxNews tonight at 9:0...</td>\n",
       "      <td>Trump</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "created_at                                                               \n",
       "2016-12-01 14:38:09  Getting ready to leave for the Great State of ...   \n",
       "2016-12-01 22:52:10  Heading to U.S. Bank Arena in Cincinnati Ohio ...   \n",
       "2016-12-02 02:45:18  Thank you Ohio! Together we made history – and...   \n",
       "2016-12-03 00:44:20  The President of Taiwan CALLED ME today to wis...   \n",
       "2016-12-03 01:41:30  Interesting how the U.S. sells Taiwan billions...   \n",
       "...                                                                ...   \n",
       "2017-03-24 17:03:46  Today I was pleased to announce the official a...   \n",
       "2017-03-24 17:59:42  Today I was thrilled to announce a commitment ...   \n",
       "2017-03-25 13:29:17  Happy #MedalOfHonorDay to our heroes! ➡️https:...   \n",
       "2017-03-25 14:37:52  ObamaCare will explode and we will all get tog...   \n",
       "2017-03-25 14:41:14  Watch @JudgeJeanine on @FoxNews tonight at 9:0...   \n",
       "\n",
       "                      is_trump  target  \n",
       "created_at                              \n",
       "2016-12-01 14:38:09      Trump       1  \n",
       "2016-12-01 22:52:10  Not Trump       0  \n",
       "2016-12-02 02:45:18  Not Trump       0  \n",
       "2016-12-03 00:44:20      Trump       1  \n",
       "2016-12-03 01:41:30      Trump       1  \n",
       "...                        ...     ...  \n",
       "2017-03-24 17:03:46  Not Trump       0  \n",
       "2017-03-24 17:59:42  Not Trump       0  \n",
       "2017-03-25 13:29:17  Not Trump       0  \n",
       "2017-03-25 14:37:52      Trump       1  \n",
       "2017-03-25 14:41:14      Trump       1  \n",
       "\n",
       "[588 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = find_trump[['text','is_trump','target']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "\n",
    "- Two Part Word2Vec Tutorial  (linked from Learn)\n",
    "    - [Part 1: The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "    - [Part 2: Negative Sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `sentences`: dataset to train on\n",
    "- `size`: how big of a word vector do we want\n",
    "- `window`: how many words around the target word to train with\n",
    "- `min_count`: how many times the word shows up in corpus; we don't want words that are rarely used\n",
    "- `workers`: number of threads (individual task \"workers\")\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Let's assume we have our text corpus already tokenized and stored inside the variable 'data'--the regular text preprocessing steps still need to be handled before training a Word2Vec model!\n",
    "\n",
    "model = Word2Vec(data, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.train(data, total_examples=model.corpus_count)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:06.391195Z",
     "start_time": "2020-11-18T23:14:06.386372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you Ohio! Together we made history – and now the real work begins. America will start winning again!… https://t.co/sVNSNJE7Uf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Thank you Ohio! Together we made history – and now the real work begins. America will start winning again!…  '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "test_str = df['text'].iloc[2]\n",
    "print(test_str)\n",
    "re.sub(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\",' ',test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:06.401540Z",
     "start_time": "2020-11-18T23:14:06.394461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you Ohio! Together we made history – and now the real work begins. America will start winning again!…  '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_urls(string): \n",
    "    return re.findall(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\",string)\n",
    "def sub_urls(string,replace_with=' '):\n",
    "    return re.sub(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\",replace_with,string)\n",
    "df['text_clean'] = df['text'].apply(lambda x: sub_urls(x,' '))\n",
    "df['text_clean'].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:06.410081Z",
     "start_time": "2020-11-18T23:14:06.404195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Boeing is building a brand new 747 Air Force One for future presidents but costs are out of control more than $4 billion. Cancel order!',\n",
       "       'We cannot continue to let Israel be treated with such total disdain and disrespect. They used to have a great friend in the U.S. but.......',\n",
       "       '@CNN just released a book called \"Unprecedented\" which explores the 2016 race &amp; victory. Hope it does well but used worst cover photo of me!',\n",
       "       'A beautiful funeral today for a real NYC hero Detective Steven McDonald. Our law enforcement community has my complete and total support.',\n",
       "       'Somebody with aptitude and conviction should buy the FAKE NEWS and failing @nytimes and either run it correctly or let it fold with dignity!',\n",
       "       \"Why aren't the lawyers looking at and using the Federal Court decision in Boston which is at conflict with ridiculous lift ban decision?\",\n",
       "       'Our legal system is broken! \"77% of refugees allowed into U.S. since travel reprieve hail from seven suspect countries.\" (WT)  SO DANGEROUS!',\n",
       "       'find the leakers within the FBI itself. Classified information is being given to media that could have a devastating effect on U.S. FIND NOW',\n",
       "       'LinkedIn Workforce Report: January and February were the strongest consecutive months for hiring since August and September 2015'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['text_clean'].str.contains('t.co')]['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:07.184941Z",
     "start_time": "2020-11-18T23:14:06.411944Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "## TRAINING WORD2VEC FROM FULL DF NOT JUST TARGETS\n",
    "data_lower = df['text_clean'].map(lambda x: simple_preprocess(x,True))#word_tokenize)\n",
    "\n",
    "#  data_lower= list(map(lambda x: [w.lower() for w in x],data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:07.195359Z",
     "start_time": "2020-11-18T23:14:07.186354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank',\n",
       " 'you',\n",
       " 'ohio',\n",
       " 'together',\n",
       " 'we',\n",
       " 'made',\n",
       " 'history',\n",
       " 'and',\n",
       " 'now',\n",
       " 'the',\n",
       " 'real',\n",
       " 'work',\n",
       " 'begins',\n",
       " 'america',\n",
       " 'will',\n",
       " 'start',\n",
       " 'winning',\n",
       " 'again']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lower[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:07.199470Z",
     "start_time": "2020-11-18T23:14:07.197110Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# text = simple_preprocess(' '.join(df['text']))\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:07.719799Z",
     "start_time": "2020-11-18T23:14:07.201242Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(data_lower,size=100,window=4,min_count=1,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:07.854239Z",
     "start_time": "2020-11-18T23:14:07.721407Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86093, 109380)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(data_lower, total_examples=model.corpus_count,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:07.857965Z",
     "start_time": "2020-11-18T23:14:07.855765Z"
    }
   },
   "outputs": [],
   "source": [
    "wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:07.868164Z",
     "start_time": "2020-11-18T23:14:07.859596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Word2VecKeyedVectors in module gensim.models.keyedvectors object:\n",
      "\n",
      "class Word2VecKeyedVectors(WordEmbeddingsKeyedVectors)\n",
      " |  Mapping between words and vectors for the :class:`~gensim.models.Word2Vec` model.\n",
      " |  Used to perform operations on the vectors such as vector lookup, distance, similarity etc.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2VecKeyedVectors\n",
      " |      WordEmbeddingsKeyedVectors\n",
      " |      BaseKeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None)\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path used to save the vectors in\n",
      " |      fvocab : str, optional\n",
      " |          Optional file path used to save the vocabulary\n",
      " |      binary : bool, optional\n",
      " |          If True, the data will be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |      total_vec : int, optional\n",
      " |          Optional parameter to explicitly specify total no. of vectors\n",
      " |          (in case word vectors are appended with document vectors afterwards).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(fname_or_handle, **kwargs) from builtins.type\n",
      " |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to file that contains needed object.\n",
      " |      mmap : str, optional\n",
      " |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      " |          via mmap (shared memory) using `mmap='r'.\n",
      " |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.save`\n",
      " |          Save object to file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object\n",
      " |          Object loaded from `fname`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          When called on an object instance instead of class (this is a class method).\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      " |      Load the input-hidden weight matrix from the original C word2vec-tool format.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      The information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to the saved word2vec-format file.\n",
      " |      fvocab : str, optional\n",
      " |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n",
      " |          (this is the file generated by `-save-vocab` flag of the original C tool).\n",
      " |      binary : bool, optional\n",
      " |          If True, indicates whether the data is in binary word2vec format.\n",
      " |      encoding : str, optional\n",
      " |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n",
      " |      unicode_errors : str, optional\n",
      " |          default 'strict', is a string suitable to be passed as the `errors`\n",
      " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      limit : int, optional\n",
      " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
      " |          None, means read all.\n",
      " |      datatype : type, optional\n",
      " |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n",
      " |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |  \n",
      " |  __init__(self, vector_size)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=<function WordEmbeddingsKeyedVectors.most_similar at 0x7fa9b9a66048>, case_insensitive=True)\n",
      " |      Compute accuracy of the model.\n",
      " |      \n",
      " |      The accuracy is reported (=printed to log and returned as a list) for each\n",
      " |      section separately, plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      questions : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      most_similar : function, optional\n",
      " |          Function used for similarity calculation.\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of dict of (str, (str, str, str)\n",
      " |          Full lists of correct and incorrect predictions divided by sections.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two words.\n",
      " |      Calculate 1 - :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Distance between `w1` and `w2`.\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : {str, numpy.ndarray}\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      other_words : iterable of str\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n",
      " |      \n",
      " |      Raises\n",
      " |      -----\n",
      " |      KeyError\n",
      " |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which word from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of words.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The word further away from the mean of all words.\n",
      " |  \n",
      " |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute performance of the model on an analogy test set.\n",
      " |      \n",
      " |      This is modern variant of :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.accuracy`, see\n",
      " |      `discussion on GitHub #1935 <https://github.com/RaRe-Technologies/gensim/pull/1935>`_.\n",
      " |      \n",
      " |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
      " |      plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      analogies : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          The overall evaluation score on the entire evaluation set\n",
      " |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
      " |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
      " |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
      " |          keys 'correct' and 'incorrect'.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      More datasets can be found at\n",
      " |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n",
      " |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pairs : str\n",
      " |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n",
      " |          See `test/test_data/wordsim353.tsv` as example.\n",
      " |      delimiter : str, optional\n",
      " |          Separator in `pairs` file.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pearson : tuple of (float, float)\n",
      " |          Pearson correlation coefficient with 2-tailed p-value.\n",
      " |      spearman : tuple of (float, float)\n",
      " |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n",
      " |          similarities produced by the model itself, with 2-tailed p-value.\n",
      " |      oov_ratio : float\n",
      " |          The ratio of pairs with unknown words.\n",
      " |  \n",
      " |  get_keras_embedding(self, train_embeddings=False, word_index=None)\n",
      " |      Get a Keras 'Embedding' layer with weights set as the Word2Vec model's learned word embeddings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      train_embeddings : bool\n",
      " |          If False, the weights are frozen and stopped from being updated.\n",
      " |          If True, the weights can/will be further trained/updated.\n",
      " |      \n",
      " |      word_index : {str : int}\n",
      " |          A mapping from tokens to their indices the way they will be provided in the input to the embedding layer.\n",
      " |          The embedding of each token will be placed at the corresponding index in the returned matrix.\n",
      " |          Tokens not in the index are ignored.\n",
      " |          This is useful when the token indices are produced by a process that is not coupled with the embedding\n",
      " |          model, e.x. an Keras Tokenizer object.\n",
      " |          If None, the embedding matrix in the embedding layer will be indexed according to self.vocab\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `keras.layers.Embedding`\n",
      " |          Embedding layer.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `Keras <https://pypi.org/project/Keras/>`_ not installed.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      Current method works only if `Keras <https://pypi.org/project/Keras/>`_ installed.\n",
      " |  \n",
      " |  get_vector(self, word)\n",
      " |      Get the entity's representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entity : str\n",
      " |          Identifier of the entity to return the vector for.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector for the specified entity.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If the given entity identifier doesn't exist.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool, optional\n",
      " |          If True - forget the original vectors and only keep the normalized ones = saves lots of memory!\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      You **cannot continue training** after doing a replace.\n",
      " |      The model becomes effectively read-only: you can call\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`,\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`, etc., but not train.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar words.\n",
      " |      Positive words contribute positively towards the similarity, negative words negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given words and the vectors for each word in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective,\n",
      " |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
      " |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
      " |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively - a potentially sensible but untested extension of the method.\n",
      " |      With a single positive example, rankings will be the same as in the default\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ws1 : list of str\n",
      " |          Sequence of words.\n",
      " |      ws2: list of str\n",
      " |          Sequence of words.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Similarities between `ws1` and `ws2`.\n",
      " |  \n",
      " |  relative_cosine_similarity(self, wa, wb, topn=10)\n",
      " |      Compute the relative cosine similarity between two words given top-n similar words,\n",
      " |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
      " |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
      " |      \n",
      " |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
      " |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
      " |      any arbitrary word pairs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      wa: str\n",
      " |          Word for which we have to look top-n similar word.\n",
      " |      wb: str\n",
      " |          Word for which we evaluating relative cosine similarity with wa.\n",
      " |      topn: int, optional\n",
      " |          Number of top-n similar words to look with respect to wa.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.float64\n",
      " |          Relative cosine similarity between wa and wb.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save KeyedVectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the output file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.load`\n",
      " |          Load saved model.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words by vector.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector : numpy.array\n",
      " |          Vector from which similarities are to be computed.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Word\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return. If topn is None, similar_by_word returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Cosine similarity between `w1` and `w2`.\n",
      " |  \n",
      " |  similarity_matrix(self, dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100, dtype=<class 'numpy.float32'>)\n",
      " |      Construct a term similarity matrix for computing Soft Cosine Measure.\n",
      " |      \n",
      " |      This creates a sparse term similarity matrix in the :class:`scipy.sparse.csc_matrix` format for computing\n",
      " |      Soft Cosine Measure between documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\n",
      " |          A dictionary that specifies the considered terms.\n",
      " |      tfidf : :class:`gensim.models.tfidfmodel.TfidfModel` or None, optional\n",
      " |          A model that specifies the relative importance of the terms in the dictionary. The\n",
      " |          columns of the term similarity matrix will be build in a decreasing order of importance\n",
      " |          of terms, or in the order of term identifiers if None.\n",
      " |      threshold : float, optional\n",
      " |          Only embeddings more similar than `threshold` are considered when retrieving word\n",
      " |          embeddings closest to a given word embedding.\n",
      " |      exponent : float, optional\n",
      " |          Take the word embedding similarities larger than `threshold` to the power of `exponent`.\n",
      " |      nonzero_limit : int, optional\n",
      " |          The maximum number of non-zero elements outside the diagonal in a single column of the\n",
      " |          sparse term similarity matrix.\n",
      " |      dtype : numpy.dtype, optional\n",
      " |          Data-type of the sparse term similarity matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`scipy.sparse.csc_matrix`\n",
      " |          Term similarity matrix.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :func:`gensim.matutils.softcossim`\n",
      " |          The Soft Cosine Measure.\n",
      " |      :class:`~gensim.similarities.docsim.SoftCosineSimilarity`\n",
      " |          A class for performing corpus-based similarity queries with Soft Cosine Measure.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The constructed matrix corresponds to the matrix Mrel defined in section 2.1 of\n",
      " |      `Delphine Charlet and Geraldine Damnati, \"SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity\n",
      " |      between Questions for Community Question Answering\", 2017\n",
      " |      <http://www.aclweb.org/anthology/S/S17/S17-2051.pdf>`_.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Compute the Word Mover's Distance between two documents.\n",
      " |      \n",
      " |      When using this code, please consider citing the following papers:\n",
      " |      \n",
      " |      * `Ofir Pele and Michael Werman \"A linear time histogram metric for improved SIFT matching\"\n",
      " |        <http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf>`_\n",
      " |      * `Ofir Pele and Michael Werman \"Fast and robust earth mover's distances\"\n",
      " |        <https://ieeexplore.ieee.org/document/5459199/>`_\n",
      " |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n",
      " |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document1 : list of str\n",
      " |          Input document.\n",
      " |      document2 : list of str\n",
      " |          Input document.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Word Mover's distance between `document1` and `document2`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method only works if `pyemd <https://pypi.org/project/pyemd/>`_ is installed.\n",
      " |      \n",
      " |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n",
      " |      will be returned.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `pyemd <https://pypi.org/project/pyemd/>`_  isn't installed.\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word\n",
      " |      use_norm : bool, optional\n",
      " |          If True - resulting vector will be L2-normalized (unit euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word not in vocabulary.\n",
      " |  \n",
      " |  words_closer_than(self, w1, w2)\n",
      " |      Get all words that are closer to `w1` than `w2` is to `w1`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list (str)\n",
      " |          List of words that are closer to `w1` than `w2` is to `w1`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Compute cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.ndarray\n",
      " |          Vector from which similarities are to be computed, expected shape (dim,).\n",
      " |      vectors_all : numpy.ndarray\n",
      " |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  index2entity\n",
      " |  \n",
      " |  syn0\n",
      " |  \n",
      " |  syn0norm\n",
      " |  \n",
      " |  wv\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseKeyedVectors:\n",
      " |  \n",
      " |  __getitem__(self, entities)\n",
      " |      Get vector representation of `entities`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : {str, list of str}\n",
      " |          Input entity/entities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation for `entities` (1D if `entities` is string, otherwise - 2D).\n",
      " |  \n",
      " |  __setitem__(self, entities, weights)\n",
      " |      Add entities and theirs vectors in a manual way.\n",
      " |      If some entity is already in the vocabulary, old vector is replaced with the new one.\n",
      " |      This method is alias for :meth:`~gensim.models.keyedvectors.BaseKeyedVectors.add` with `replace=True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : {str, list of str}\n",
      " |          Entities specified by their string ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or 2D np.array of vectors.\n",
      " |  \n",
      " |  add(self, entities, weights, replace=False)\n",
      " |      Append entities and theirs vectors in a manual way.\n",
      " |      If some entity is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : list of str\n",
      " |          Entities specified by string ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or a 2D np.array of vectors.\n",
      " |      replace: bool, optional\n",
      " |          Flag indicating whether to replace vectors for entities which already exist in the vocabulary,\n",
      " |          if True - replace vectors, otherwise - keep old vectors.\n",
      " |  \n",
      " |  closer_than(self, entity1, entity2)\n",
      " |      Get all entities that are closer to `entity1` than `entity2` is to `entity1`.\n",
      " |  \n",
      " |  most_similar_to_given(self, entity1, entities_list)\n",
      " |      Get the `entity` from `entities_list` most similar to `entity1`.\n",
      " |  \n",
      " |  rank(self, entity1, entity2)\n",
      " |      Rank of the distance of `entity2` from `entity1`, in relation to distances of all entities from `entity1`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:07.874202Z",
     "start_time": "2020-11-18T23:14:07.869752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.16146585,  0.09873124,  0.02179674,  0.10310663, -0.08975983,\n",
       "       -0.14740214,  0.01140198, -0.02606054,  0.10334659, -0.03482863,\n",
       "        0.03429934, -0.03383731,  0.06061233,  0.12548216, -0.00743585,\n",
       "       -0.07311445, -0.00317723,  0.03559231,  0.02683255, -0.1384502 ,\n",
       "       -0.11879463, -0.05856081,  0.06768795,  0.02903818,  0.01028467,\n",
       "       -0.01999044,  0.07749817,  0.00108099, -0.04796066, -0.03504924,\n",
       "       -0.02595687,  0.177367  ,  0.07052595, -0.13404928,  0.04207534,\n",
       "       -0.03471836,  0.10900465,  0.12797906, -0.02772324, -0.09520952,\n",
       "       -0.00172614, -0.00811145, -0.09460182, -0.08646218, -0.06296798,\n",
       "       -0.10063058, -0.02381663,  0.02900884, -0.1408034 , -0.01956715,\n",
       "        0.02615057, -0.04137802, -0.10741008, -0.04708083, -0.00287064,\n",
       "        0.02982791, -0.03173379, -0.07823544,  0.02305234, -0.07173935,\n",
       "       -0.05388825, -0.03104883, -0.10245232, -0.0098181 , -0.0757836 ,\n",
       "        0.0290911 , -0.0159634 ,  0.04492551, -0.04201635, -0.07083497,\n",
       "        0.00494626, -0.02327331, -0.07717834,  0.03270706, -0.02135266,\n",
       "       -0.07261893, -0.02746321,  0.0181754 ,  0.12834595, -0.1171547 ,\n",
       "        0.04018413,  0.061274  , -0.03611865,  0.02070943,  0.01036751,\n",
       "       -0.085104  ,  0.05404803,  0.04117369, -0.13461858,  0.09545916,\n",
       "        0.07842787,  0.00580601, -0.07784189,  0.00912023,  0.00119406,\n",
       "        0.02332782,  0.05927165, -0.05192016, -0.04889918,  0.02477164],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['republican']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:07.881328Z",
     "start_time": "2020-11-18T23:14:07.875960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 0.9991688132286072),\n",
       " ('the', 0.9991651177406311),\n",
       " ('president', 0.9991632103919983),\n",
       " ('as', 0.9991621971130371),\n",
       " ('meeting', 0.9991601705551147),\n",
       " ('an', 0.9991456270217896),\n",
       " ('obama', 0.9991408586502075),\n",
       " ('would', 0.9991399645805359),\n",
       " ('pm', 0.9991357922554016),\n",
       " ('in', 0.9991302490234375)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('republican')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:07.888184Z",
     "start_time": "2020-11-18T23:14:07.883458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meltdown', 0.8356411457061768),\n",
       " ('club', 0.8350019454956055),\n",
       " ('responded', 0.8057776689529419),\n",
       " ('proof', 0.6644962430000305),\n",
       " ('feds', 0.5440253615379333),\n",
       " ('secretly', 0.28709185123443604),\n",
       " ('explode', 0.2695140838623047),\n",
       " ('doubt', -0.01369890570640564),\n",
       " ('merrychristmas', -0.06014178320765495),\n",
       " ('interfere', -0.07003508508205414)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(negative=['democrat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "## For initializing model\n",
    "sentences=None,\n",
    "    size=100,\n",
    "    alpha=0.025,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    max_vocab_size=None,\n",
    "    sample=0.001,\n",
    "    seed=1,\n",
    "    workers=3,\n",
    "    min_alpha=0.0001,\n",
    "    sg=0,\n",
    "    hs=0,\n",
    "    negative=5,\n",
    "    cbow_mean=1,\n",
    "    hashfxn=<built-in function hash>,\n",
    "    iter=5,\n",
    "    null_word=0,\n",
    "    trim_rule=None,\n",
    "    sorted_vocab=1,\n",
    "    batch_words=10000,\n",
    "    compute_loss=False,\n",
    "    callbacks=(),\n",
    "    \n",
    "    \n",
    "## For training \n",
    "    sentences,\n",
    "    total_examples=None,\n",
    "    total_words=None,\n",
    "    epochs=None,\n",
    "    start_alpha=None,\n",
    "    end_alpha=None,\n",
    "    word_count=0,\n",
    "    queue_factor=2,\n",
    "    report_delay=1.0,\n",
    "    compute_loss=False,\n",
    "    callbacks=(),\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:07.895472Z",
     "start_time": "2020-11-18T23:14:07.889970Z"
    }
   },
   "outputs": [],
   "source": [
    "# ### USING WORD VECTOR MATH TO GET A FEEL FOR QUALITY OF MODE\n",
    "def word_math(wv,pos_words=['hillary'],neg_words=['bill'],\n",
    "              verbose=True,return_vec=False):\n",
    "    if isinstance(pos_words,str):\n",
    "        pos_words=[pos_words]\n",
    "    if isinstance(neg_words,str):\n",
    "        neg_words=[neg_words]\n",
    "\n",
    "\n",
    "    pos_eqn = '+'.join(pos_words)\n",
    "    neg_eqn = '-'.join(neg_words)\n",
    "\n",
    "    print('---'*15)    \n",
    "    print(f\"[i] Result for:\\t{pos_eqn}{' - '+neg_eqn if len(neg_eqn)>0 else ' '}\")\n",
    "    print('---'*15)\n",
    "\n",
    "    answer = wv.most_similar(positive=pos_words,negative=neg_words)\n",
    "    \n",
    "    if verbose:\n",
    "          [print(f\"- {ans[0]} ({round(ans[1],3)})\") for ans in answer]\n",
    "          print('---'*15,'\\n\\n')\n",
    "\n",
    "    if return_vec: \n",
    "          return answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:07.907619Z",
     "start_time": "2020-11-18T23:14:07.897550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "[i] Result for:\tamerica+crime \n",
      "---------------------------------------------\n",
      "- we (1.0)\n",
      "- great (1.0)\n",
      "- to (1.0)\n",
      "- who (1.0)\n",
      "- meeting (1.0)\n",
      "- will (1.0)\n",
      "- for (1.0)\n",
      "- am (1.0)\n",
      "- amp (1.0)\n",
      "- new (1.0)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\tdemocrats+russia \n",
      "---------------------------------------------\n",
      "- in (1.0)\n",
      "- by (1.0)\n",
      "- was (1.0)\n",
      "- just (1.0)\n",
      "- from (1.0)\n",
      "- the (1.0)\n",
      "- and (1.0)\n",
      "- to (1.0)\n",
      "- that (1.0)\n",
      "- for (1.0)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\trepublican - honor\n",
      "---------------------------------------------\n",
      "- interfere (0.167)\n",
      "- chosen (0.111)\n",
      "- pressing (0.099)\n",
      "- clearly (0.092)\n",
      "- explode (0.08)\n",
      "- powerful (0.077)\n",
      "- speed (0.077)\n",
      "- thr (0.077)\n",
      "- rapidly (0.076)\n",
      "- feds (0.074)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\tman+power \n",
      "---------------------------------------------\n",
      "- january (0.997)\n",
      "- schumer (0.997)\n",
      "- world (0.997)\n",
      "- this (0.997)\n",
      "- do (0.997)\n",
      "- always (0.997)\n",
      "- must (0.997)\n",
      "- total (0.997)\n",
      "- united (0.997)\n",
      "- most (0.997)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\trussia+honor \n",
      "---------------------------------------------\n",
      "- the (1.0)\n",
      "- many (1.0)\n",
      "- now (1.0)\n",
      "- of (1.0)\n",
      "- will (1.0)\n",
      "- not (1.0)\n",
      "- to (1.0)\n",
      "- by (1.0)\n",
      "- amp (1.0)\n",
      "- new (1.0)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\tchina - tariff\n",
      "---------------------------------------------\n",
      "- merrychristmas (0.157)\n",
      "- bikers (0.15)\n",
      "- interfere (0.141)\n",
      "- humiliating (0.114)\n",
      "- obstruct (0.114)\n",
      "- turns (0.114)\n",
      "- pressing (0.113)\n",
      "- flunky (0.11)\n",
      "- spotlight (0.107)\n",
      "- reference (0.106)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "equation_list=[(['america','crime'],[]),\n",
    "               \n",
    "               (['democrats','russia'],[]),\n",
    "               (['republican'],['honor']),\n",
    "               (['man','power'],[]),\n",
    "               (['russia','honor'],[]),\n",
    "              (['china','tariff'])]\n",
    "\n",
    "for eqn in equation_list:\n",
    "#     print('\\n\\n')\n",
    "    word_math(wv,*eqn)\n",
    "#     word_math(wv2,*eqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe - Global Vectors for Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usually embeddings are hundreds of dimensions\n",
    "- Just use the word embeddings already learned from before!\n",
    "    + Unless very specific terminology, context will likely carry within language\n",
    "- Comparable to CNN transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Models - ANNs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-sequence-model-use-cases-online-ds-ft-100719/master/images/rnn.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-understanding-recurrent-neural-networks-online-ds-ft-100719/master/images/unrolled.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-sequence-model-use-cases-online-ds-ft-100719/master/images/RNN-unrolled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMs GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GRU (Gated Recurrent Units (GRUs)\n",
    "    - Reset Gate\n",
    "    - Update Gate\n",
    "    \n",
    "- LSTM (Long Short Term Memory Cells)\n",
    "   - Input Gate\n",
    "   - Forget Gate\n",
    "   - Output Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Embeddings in Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layers\n",
    "You should make note of a couple caveats that come with using embedding layers in your neural network -- namely:\n",
    "\n",
    "* The embedding layer must always be the first layer of the network, meaning that it should immediately follow the `Input()` layer \n",
    "* All words in the text should be integer-encoded, with each unique word encoded as it's own unique integer  \n",
    "* The size of the embedding layer must always be greater than the total vocabulary size of the dataset! The first parameter denotes the vocabulary size, while the second denotes the size of the actual word vectors\n",
    "* The size of the sequences passed in as data must be set when creating the layer (all data will be converted to padded sequences of the same size during the preprocessing step) \n",
    "\n",
    "\n",
    "[Keras Documentation for Embedding Layers](https://keras.io/layers/embeddings/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pre-Trained Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:07.912957Z",
     "start_time": "2020-11-18T23:14:07.909647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jamesirving/Datasets/glove.6B/glove.6B.50d.txt\n",
      "/Users/jamesirving/Datasets/glove.twitter.27B/glove.twitter.27B.50d.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "folder = '/Users/jamesirving/Datasets/'#glove.twitter.27B/'\n",
    "# print(os.listdir(folder))\n",
    "glove_file = folder+'glove.6B/glove.6B.50d.txt'#'glove.twitter.27B.50d.txt'\n",
    "glove_twitter_file = folder+'glove.twitter.27B/glove.twitter.27B.50d.txt'\n",
    "print(glove_file)\n",
    "print(glove_twitter_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping only the vectors needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:07.926880Z",
     "start_time": "2020-11-18T23:14:07.922259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2337"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This line of code for getting all words bugs me\n",
    "total_vocabulary = set(word for tweet in data_lower for word in tweet)\n",
    "len(total_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:09.075605Z",
     "start_time": "2020-11-18T23:14:07.932143Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open(glove_file,'rb') as f:#'glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:09.081662Z",
     "start_time": "2020-11-18T23:14:09.077764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.62964 , -0.41852 ,  1.0236  , -0.20629 ,  0.18681 ,  1.1836  ,\n",
       "       -1.1125  ,  0.061848, -0.98647 , -1.1978  , -2.0101  , -0.81715 ,\n",
       "       -0.088324,  0.2431  , -0.94952 , -0.62524 ,  0.56778 , -0.66119 ,\n",
       "        0.23107 , -0.732   , -0.75439 , -0.54928 ,  0.3512  , -0.44388 ,\n",
       "       -0.28215 , -2.4233  , -0.15651 , -0.31636 , -0.45465 ,  0.63483 ,\n",
       "        2.0342  , -0.033718, -1.1777  , -1.2411  , -0.32364 , -1.0717  ,\n",
       "       -1.0004  ,  0.28567 , -0.45446 , -0.9771  , -0.70622 ,  0.85814 ,\n",
       "       -1.5202  ,  0.23351 , -0.38033 ,  0.2903  , -1.2494  , -0.17779 ,\n",
       "       -0.31254 ,  1.2733  ], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['republican']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting glove into w2vec format:\n",
    "    - https://radimrehurek.com/gensim/scripts/glove2word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:09.086579Z",
     "start_time": "2020-11-18T23:14:09.082982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glove.twitter.27B.100d.txt',\n",
       " 'glove.twitter.27B.50d.txt',\n",
       " 'glove_to_w2vec.txt',\n",
       " 'glove.twitter.27B.25d.txt',\n",
       " 'glove.twitter.27B.200d.txt']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_folder = folder+'glove.twitter.27B'\n",
    "os.listdir(glove_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:14:09.090767Z",
     "start_time": "2020-11-18T23:14:09.087954Z"
    }
   },
   "outputs": [],
   "source": [
    "del glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:03.102048Z",
     "start_time": "2020-11-18T23:14:09.092358Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath(glove_twitter_file)\n",
    "tmp_file = get_tmpfile(glove_folder+'glove_to_w2vec.txt')\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:03.107156Z",
     "start_time": "2020-11-18T23:15:03.103573Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7fa9ba0767b8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:03.746179Z",
     "start_time": "2020-11-18T23:15:03.109561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "[i] Result for:\tamerica+crime \n",
      "---------------------------------------------\n",
      "- american (0.849)\n",
      "- country (0.811)\n",
      "- criminal (0.805)\n",
      "- society (0.8)\n",
      "- africa (0.794)\n",
      "- death (0.793)\n",
      "- the (0.792)\n",
      "- world (0.789)\n",
      "- states (0.788)\n",
      "- attack (0.784)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\tdemocrats+russia \n",
      "---------------------------------------------\n",
      "- republicans (0.869)\n",
      "- conservatives (0.841)\n",
      "- ukraine (0.839)\n",
      "- government (0.835)\n",
      "- americans (0.831)\n",
      "- liberals (0.821)\n",
      "- britain (0.816)\n",
      "- u.s. (0.814)\n",
      "- immigration (0.813)\n",
      "- states (0.81)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\trepublican - honor\n",
      "---------------------------------------------\n",
      "- tsx (0.656)\n",
      "- trinamool (0.654)\n",
      "- nesunan (0.653)\n",
      "- mantashe (0.639)\n",
      "- mutungan (0.637)\n",
      "- مقدسي (0.634)\n",
      "- nyambungan (0.633)\n",
      "- مسئولون (0.628)\n",
      "- melumelu (0.628)\n",
      "- yeek (0.626)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\tman+power \n",
      "---------------------------------------------\n",
      "- the (0.861)\n",
      "- that (0.858)\n",
      "- way (0.846)\n",
      "- bad (0.844)\n",
      "- of (0.841)\n",
      "- it (0.837)\n",
      "- is (0.831)\n",
      "- but (0.829)\n",
      "- will (0.827)\n",
      "- and (0.825)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\trussia+honor \n",
      "---------------------------------------------\n",
      "- america (0.841)\n",
      "- national (0.793)\n",
      "- canada (0.789)\n",
      "- union (0.787)\n",
      "- president (0.785)\n",
      "- army (0.773)\n",
      "- us (0.769)\n",
      "- freedom (0.768)\n",
      "- africa (0.766)\n",
      "- world (0.764)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\tchina - tariff\n",
      "---------------------------------------------\n",
      "- : (0.654)\n",
      "- “ (0.645)\n",
      "- \" (0.63)\n",
      "- <user> (0.622)\n",
      "- san (0.615)\n",
      "- argentina (0.605)\n",
      "- chico (0.603)\n",
      "- mario (0.595)\n",
      "- rey (0.593)\n",
      "- raja (0.591)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Using pre-trained embeddings for math\n",
    "equation_list=[(['america','crime'],[]),\n",
    "               \n",
    "               (['democrats','russia'],[]),\n",
    "               (['republican'],['honor']),\n",
    "               (['man','power'],[]),\n",
    "               (['russia','honor'],[]),\n",
    "              (['china','tariff'])]\n",
    "\n",
    "for eqn in equation_list:\n",
    "#     print('\\n\\n')\n",
    "    word_math(model,*eqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Embeddings in Classification Models - sci-kit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embeddings can be used in Artificial Neural Networks as an input Embedding Layer\n",
    "- Embeddings can be used in sci-kit learn models by taking the mean vector of a text/document and using the mean vector as the input into the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Finding Trump Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the beginning of his presidency, Trump continued to use a non-secure Android phone for his personal use.\n",
    "- Tweets from this period can be attributed to Trump or his staffers based on if it came from an Android device or an iPhone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:03.751199Z",
     "start_time": "2020-11-18T23:15:03.748673Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"../trump_tweets_12012016_to_01012020.csv\",\n",
    "#                 parse_dates=['created_at'],index_col='created_at')#'https://raw.githubusercontent.com/jirvingphd/capstone-project-using-trumps-tweets-to-predict-stock-market/master/data/trump_tweets_12012016_to_01012020.csv')\n",
    "# # df['datetime'] = pd.to_datetime(df['created_at'])\n",
    "# df = df.sort_index()#.set_index('datetime')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:03.756111Z",
     "start_time": "2020-11-18T23:15:03.753693Z"
    }
   },
   "outputs": [],
   "source": [
    "# devices = ['Twitter for Android','Twitter for iPhone']\n",
    "# print(f'The first and last timestamps for the group {devices[0]}are:')\n",
    "# start,end= df.groupby('source').get_group(devices[0]).index[[0,-1]]\n",
    "# print(start) ,print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:03.761130Z",
     "start_time": "2020-11-18T23:15:03.758674Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Puttig it all together\n",
    "# df_data = df[df['source'].isin(devices)].loc[start:end].copy()\n",
    "# df_data['source'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:03.766029Z",
     "start_time": "2020-11-18T23:15:03.763389Z"
    }
   },
   "outputs": [],
   "source": [
    "# undersample = False\n",
    "\n",
    "# if undersample:\n",
    "#     ## Undersampling to match class\n",
    "#     iphone = df_data.loc[df_data['source']=='Twitter for iPhone']\n",
    "#     android = df_data.loc[df_data['source']=='Twitter for Android']\n",
    "#     print(len(iphone),len(android))\n",
    "#     df = pd.concat([iphone, \n",
    "#                     android.sample(n=len(iphone),\n",
    "#                                    random_state=123)],\n",
    "#                     axis=0)\n",
    "#     print('Data has been undersampled to match mintority class.')\n",
    "# else:\n",
    "#     print('No resampling was done.')\n",
    "#     df= df_data\n",
    "    \n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:03.771081Z",
     "start_time": "2020-11-18T23:15:03.768597Z"
    }
   },
   "outputs": [],
   "source": [
    "# # df.to_csv('datasets/trump_tweets_iphone_vs_twitter.csv',index=False)\n",
    "# df= pd.read_csv('datasets/trump_tweets_iphone_vs_twitter.csv')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:03.775806Z",
     "start_time": "2020-11-18T23:15:03.773267Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Embedding Layers in ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.081361Z",
     "start_time": "2020-11-18T23:15:03.777720Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.086695Z",
     "start_time": "2020-11-18T23:15:07.083005Z"
    }
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append(\"../\")\n",
    "import keras_gridsearch as kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.108150Z",
     "start_time": "2020-11-18T23:15:07.088309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_trump</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-12-01 14:38:09</th>\n",
       "      <td>Getting ready to leave for the Great State of ...</td>\n",
       "      <td>Trump</td>\n",
       "      <td>1</td>\n",
       "      <td>Getting ready to leave for the Great State of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01 22:52:10</th>\n",
       "      <td>Heading to U.S. Bank Arena in Cincinnati Ohio ...</td>\n",
       "      <td>Not Trump</td>\n",
       "      <td>0</td>\n",
       "      <td>Heading to U.S. Bank Arena in Cincinnati Ohio ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-02 02:45:18</th>\n",
       "      <td>Thank you Ohio! Together we made history – and...</td>\n",
       "      <td>Not Trump</td>\n",
       "      <td>0</td>\n",
       "      <td>Thank you Ohio! Together we made history – and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-03 00:44:20</th>\n",
       "      <td>The President of Taiwan CALLED ME today to wis...</td>\n",
       "      <td>Trump</td>\n",
       "      <td>1</td>\n",
       "      <td>The President of Taiwan CALLED ME today to wis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-03 01:41:30</th>\n",
       "      <td>Interesting how the U.S. sells Taiwan billions...</td>\n",
       "      <td>Trump</td>\n",
       "      <td>1</td>\n",
       "      <td>Interesting how the U.S. sells Taiwan billions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-24 17:03:46</th>\n",
       "      <td>Today I was pleased to announce the official a...</td>\n",
       "      <td>Not Trump</td>\n",
       "      <td>0</td>\n",
       "      <td>Today I was pleased to announce the official a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-24 17:59:42</th>\n",
       "      <td>Today I was thrilled to announce a commitment ...</td>\n",
       "      <td>Not Trump</td>\n",
       "      <td>0</td>\n",
       "      <td>Today I was thrilled to announce a commitment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-25 13:29:17</th>\n",
       "      <td>Happy #MedalOfHonorDay to our heroes! ➡️https:...</td>\n",
       "      <td>Not Trump</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy #MedalOfHonorDay to our heroes! ➡️</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-25 14:37:52</th>\n",
       "      <td>ObamaCare will explode and we will all get tog...</td>\n",
       "      <td>Trump</td>\n",
       "      <td>1</td>\n",
       "      <td>ObamaCare will explode and we will all get tog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-25 14:41:14</th>\n",
       "      <td>Watch @JudgeJeanine on @FoxNews tonight at 9:0...</td>\n",
       "      <td>Trump</td>\n",
       "      <td>1</td>\n",
       "      <td>Watch @JudgeJeanine on @FoxNews tonight at 9:0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "created_at                                                               \n",
       "2016-12-01 14:38:09  Getting ready to leave for the Great State of ...   \n",
       "2016-12-01 22:52:10  Heading to U.S. Bank Arena in Cincinnati Ohio ...   \n",
       "2016-12-02 02:45:18  Thank you Ohio! Together we made history – and...   \n",
       "2016-12-03 00:44:20  The President of Taiwan CALLED ME today to wis...   \n",
       "2016-12-03 01:41:30  Interesting how the U.S. sells Taiwan billions...   \n",
       "...                                                                ...   \n",
       "2017-03-24 17:03:46  Today I was pleased to announce the official a...   \n",
       "2017-03-24 17:59:42  Today I was thrilled to announce a commitment ...   \n",
       "2017-03-25 13:29:17  Happy #MedalOfHonorDay to our heroes! ➡️https:...   \n",
       "2017-03-25 14:37:52  ObamaCare will explode and we will all get tog...   \n",
       "2017-03-25 14:41:14  Watch @JudgeJeanine on @FoxNews tonight at 9:0...   \n",
       "\n",
       "                      is_trump  target  \\\n",
       "created_at                               \n",
       "2016-12-01 14:38:09      Trump       1   \n",
       "2016-12-01 22:52:10  Not Trump       0   \n",
       "2016-12-02 02:45:18  Not Trump       0   \n",
       "2016-12-03 00:44:20      Trump       1   \n",
       "2016-12-03 01:41:30      Trump       1   \n",
       "...                        ...     ...   \n",
       "2017-03-24 17:03:46  Not Trump       0   \n",
       "2017-03-24 17:59:42  Not Trump       0   \n",
       "2017-03-25 13:29:17  Not Trump       0   \n",
       "2017-03-25 14:37:52      Trump       1   \n",
       "2017-03-25 14:41:14      Trump       1   \n",
       "\n",
       "                                                            text_clean  \n",
       "created_at                                                              \n",
       "2016-12-01 14:38:09  Getting ready to leave for the Great State of ...  \n",
       "2016-12-01 22:52:10  Heading to U.S. Bank Arena in Cincinnati Ohio ...  \n",
       "2016-12-02 02:45:18  Thank you Ohio! Together we made history – and...  \n",
       "2016-12-03 00:44:20  The President of Taiwan CALLED ME today to wis...  \n",
       "2016-12-03 01:41:30  Interesting how the U.S. sells Taiwan billions...  \n",
       "...                                                                ...  \n",
       "2017-03-24 17:03:46  Today I was pleased to announce the official a...  \n",
       "2017-03-24 17:59:42  Today I was thrilled to announce a commitment ...  \n",
       "2017-03-25 13:29:17        Happy #MedalOfHonorDay to our heroes! ➡️     \n",
       "2017-03-25 14:37:52  ObamaCare will explode and we will all get tog...  \n",
       "2017-03-25 14:41:14  Watch @JudgeJeanine on @FoxNews tonight at 9:0...  \n",
       "\n",
       "[588 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.115871Z",
     "start_time": "2020-11-18T23:15:07.110101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import text,sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn import metrics\n",
    "\n",
    "X = df['text']\n",
    "# y_t = pd.get_dummies(df['source'])#.values\n",
    "# y_t=y_t['Twitter for Android'].values\n",
    "y_t = to_categorical(df['target'])#.map({'Twitter for Android':1,\n",
    "#                                        'Twitter for iPhone':0}),num_classes=2)\n",
    "\n",
    "# y_t.name='True Trump'\n",
    "\n",
    "# print(y_t.name)\n",
    "y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.156669Z",
     "start_time": "2020-11-18T23:15:07.117672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(588, 50)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_WORDS = 25000\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=MAX_WORDS)\n",
    "\n",
    "tokenizer.fit_on_texts(X) #df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(X) #df['text'])\n",
    "\n",
    "X_t = sequence.pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.161879Z",
     "start_time": "2020-11-18T23:15:07.158433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,  169, 2705,   16,  134,\n",
       "         95,   20,  211,   75,   91,   77], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.168633Z",
     "start_time": "2020-11-18T23:15:07.163503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   1, 223, 338],\n",
       "       [  0,   0,   0, ...,  82, 152,  57],\n",
       "       [  0,   0,   0, ...,  75,  91,  77],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,  98, 300, 665],\n",
       "       [  0,   0,   0, ...,  55,   2, 463],\n",
       "       [  0,   0,   0, ...,  92,  60,   3]], dtype=int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(X_t,y_t,random_state=123) \n",
    "X_train.shape,y_test.shape\n",
    "# pd.Series(y_test).value_counts(normalize=True)\n",
    "y_test.shape\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.172416Z",
     "start_time": "2020-11-18T23:15:07.170293Z"
    }
   },
   "outputs": [],
   "source": [
    "# EMBEDDING_SIZE = 128 #where codealong get this?\n",
    "\n",
    "# embedding_matrix = np.zeros((len(total_vocabulary) + 1, EMBEDDING_SIZE))\n",
    "# for word, i in enumerate(total_vocabulary):#.items():\n",
    "#     embedding_vector = glove.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "        \n",
    "# embedding_layer = Embedding(len(total_vocabulary) + 1,\n",
    "#                             EMBEDDING_SIZE,\n",
    "#                             weights=[embedding_matrix],\n",
    "#                             input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                             trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.938789Z",
     "start_time": "2020-11-18T23:15:07.174045Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EMBEDDING_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-d6c15369775f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_WORDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGlobalMaxPool1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EMBEDDING_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "# EMBEDDING_SIZE = 128 #where codealong get this?\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(MAX_WORDS, EMBEDDING_SIZE))\n",
    "model.add(LSTM(25,return_sequences=True))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',#'categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_hat_test = model.predict_classes(X_test)\n",
    "# print(pd.Series(y_hat_test).value_counts())\n",
    "kg.evaluate_model(y_test,y_hat_test,history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.942703Z",
     "start_time": "2020-11-18T23:13:58.786Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(MAX_WORDS, EMBEDDING_SIZE))\n",
    "# model.add(LSTM(output_dim=256, activation='sigmoid', recurrent_activation='hard_sigmoid', return_sequences=True))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(LSTM(output_dim=256, activation='sigmoid', recurrent_activation='hard_sigmoid'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.943852Z",
     "start_time": "2020-11-18T23:13:58.790Z"
    }
   },
   "outputs": [],
   "source": [
    "y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:18:01.708621Z",
     "start_time": "2020-02-17T00:18:00.772373Z"
    }
   },
   "source": [
    "## RNN or GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.945056Z",
     "start_time": "2020-11-18T23:13:58.795Z"
    }
   },
   "outputs": [],
   "source": [
    "## GRU Model\n",
    "from keras import models, layers, optimizers, regularizers\n",
    "modelG = models.Sequential()\n",
    "\n",
    "## Get and add embedding_layer\n",
    "# embedding_layer = ji.make_keras_embedding_layer(wv, X_train)\n",
    "modelG.add(Embedding(MAX_WORDS, EMBEDDING_SIZE))\n",
    "\n",
    "# modelG.add(layers.SpatialDropout1D(0.5))\n",
    "# modelG.add(layers.Bidirectional(layers.GRU(units=100, dropout=0.5, recurrent_dropout=0.2,return_sequences=True)))\n",
    "modelG.add(layers.Bidirectional(layers.GRU(units=100, dropout=0.5, recurrent_dropout=0.2)))\n",
    "modelG.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "modelG.compile(loss='categorical_crossentropy',optimizer=\"adam\",metrics=['acc'])#,'val_acc'])#, callbacks=callbacks)\n",
    "modelG.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.946162Z",
     "start_time": "2020-11-18T23:13:58.800Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "history = modelG.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_hat_test = modelG.predict_classes(X_test)\n",
    "kg.evaluate_model(y_test,y_hat_test,history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.947320Z",
     "start_time": "2020-11-18T23:13:58.804Z"
    }
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Mean Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.948402Z",
     "start_time": "2020-11-18T23:13:58.810Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "\n",
    "y = pd.get_dummies(df['source'],drop_first=True).values\n",
    "X = df['text'].str.lower().map(word_tokenize)\n",
    "\n",
    "X_idx = list(range(len(X)))\n",
    "train_idx,test_idx = train_test_split(X_idx,random_state=123)\n",
    "\n",
    "X[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.949497Z",
     "start_time": "2020-11-18T23:13:58.815Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split_idx(X, y, train_idx,test_idx):\n",
    "    # try count vectorized first\n",
    "    X_train = X[train_idx].copy()\n",
    "    y_train = y[train_idx].copy()\n",
    "    X_test = X[train_idx].copy()\n",
    "    y_test = y[train_idx].copy()\n",
    "    return X_train, X_test,y_train, y_test\n",
    "\n",
    "X_train, X_test,y_train, y_test = train_test_split_idx(X,y,train_idx,test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.950506Z",
     "start_time": "2020-11-18T23:13:58.819Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['combined_text'] = df['headline'] + ' ' + df['short_description']\n",
    "# data = df['combined_text'].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.951538Z",
     "start_time": "2020-11-18T23:13:58.823Z"
    }
   },
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # Takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # it can't be used in a scikit-learn pipeline  \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.952463Z",
     "start_time": "2020-11-18T23:13:58.827Z"
    }
   },
   "outputs": [],
   "source": [
    "# target = df['source']\n",
    "# data = df['text'].map(word_tokenize)\n",
    "# data_lower = list(map(lambda x: [w.lower() for w in x],data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.953402Z",
     "start_time": "2020-11-18T23:13:58.830Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])\n",
    "\n",
    "models = [('Random Forest', rf),\n",
    "          ('Support Vector Machine', svc),\n",
    "          ('Logistic Regression', lr)]\n",
    "# models = {'Random Forest':RandomForestClassifier(n_estimators=100, verbose=True),\n",
    "#           'SVC':SVC(),'lr':LogisticRegression()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.954590Z",
     "start_time": "2020-11-18T23:13:58.834Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = [(name, cross_val_score(model, X_train, y_train, cv=2).mean()) for name, model, in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.955778Z",
     "start_time": "2020-11-18T23:13:58.838Z"
    }
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn vectorization\n",
    "WIP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.956783Z",
     "start_time": "2020-11-18T23:13:58.843Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "tf_transformer = TfidfTransformer(use_idf=False)#TfidfTransformer()\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True)#TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.957876Z",
     "start_time": "2020-11-18T23:13:58.846Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le= LabelEncoder()\n",
    "y = le.fit_transform(df['source'])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.958990Z",
     "start_time": "2020-11-18T23:13:58.851Z"
    }
   },
   "outputs": [],
   "source": [
    "X = count_vectorizer.fit_transform(df['text'])\n",
    "X_tf = tf_transformer.fit_transform(X)\n",
    "X_tfidf = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.960129Z",
     "start_time": "2020-11-18T23:13:58.855Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tf.shape,X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.961203Z",
     "start_time": "2020-11-18T23:13:58.858Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_idx = list(range(X.shape[0]))\n",
    "train_idx,test_idx = train_test_split(X_idx,random_state=123)\n",
    "\n",
    "\n",
    "def train_test_split_idx(X, y, train_idx,test_idx):\n",
    "    # try count vectorized first\n",
    "    X_train = X[train_idx].copy()\n",
    "    y_train = y[train_idx].copy()\n",
    "    X_test = X[train_idx].copy()\n",
    "    y_test = y[train_idx].copy()\n",
    "    return X_train, X_test,y_train, y_test\n",
    "\n",
    "X_dict = {'count':X_tf,\n",
    "         'tfidf':X_tfidf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.962327Z",
     "start_time": "2020-11-18T23:13:58.863Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "# svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "#                 ('Support Vector Machine', SVC())])\n",
    "\n",
    "# lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "#               ('Logistic Regression', LogisticRegression())])\n",
    "\n",
    "# models = {'Random Forest':RandomForestClassifier(n_estimators=100, verbose=True),\n",
    "#           'SVC':SVC(),'lr':LogisticRegression()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.963387Z",
     "start_time": "2020-11-18T23:13:58.867Z"
    }
   },
   "outputs": [],
   "source": [
    "# res = [['Method','Model',\"Result\"]]\n",
    "\n",
    "# for tf_type,X_data in X_dict.items():\n",
    "#     X_train, X_test,y_train, y_test = train_test_split_idx(X_data,y,train_idx,test_idx)\n",
    "    \n",
    "#     for name, model in models.items():\n",
    "    \n",
    "# #     rf = RandomForestClassifier(n_estimators=100,verbose=True)\n",
    "#         cv_res = cross_val_score(model, X_train,y_train, cv=5)\n",
    "#         res.append([tf_type,name,cv_res.mean()])\n",
    "\n",
    "# pd.DataFrame(res[1:],columns=res[0]).sort_values(\"Result\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.964472Z",
     "start_time": "2020-11-18T23:13:58.871Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# rf_params = dict(n_estimators=100, verbose=True)\n",
    "\n",
    "# rf =Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "#               ('Random Forest',RandomForestClassifier(**rf_params))])\n",
    "\n",
    "# svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "#                 ('Support Vector Machine', SVC())])\n",
    "\n",
    "# lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "#               ('Logistic Regression', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.965478Z",
     "start_time": "2020-11-18T23:13:58.874Z"
    }
   },
   "outputs": [],
   "source": [
    "# models = [('Random Forest', rf),\n",
    "#           ('Support Vector Machine', svc),\n",
    "#           ('Logistic Regression', lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.966642Z",
     "start_time": "2020-11-18T23:13:58.878Z"
    }
   },
   "outputs": [],
   "source": [
    "# # res = [['Model','Score']]\n",
    "# res=[['Model','Scores']]\n",
    "# for (name, model) in models:\n",
    "#     print(name)\n",
    "#     cv_res = cross_val_score(model, data_lower, df['source'], cv=5).mean()\n",
    "#     res.append([name,cv_res])\n",
    "    \n",
    "# pd.DataFrame(res[1:],columns=res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practicing Text Preprocessing with Trump's Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAST CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.967755Z",
     "start_time": "2020-11-18T23:13:58.884Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a list of stopwords to remove\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.968741Z",
     "start_time": "2020-11-18T23:13:58.888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all the stop words in the English language\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list+=string.punctuation\n",
    "print(stopwords_list)\n",
    "stopwords_list.remove('until')\n",
    "stopwords_list.extend(['“','...','”'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.969748Z",
     "start_time": "2020-11-18T23:13:58.892Z"
    }
   },
   "outputs": [],
   "source": [
    "## Commentary on not always accepting what is or isn't in stopwords\n",
    "'until' in stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.970741Z",
     "start_time": "2020-11-18T23:13:58.897Z"
    }
   },
   "outputs": [],
   "source": [
    "stopped_tokens = [w.lower() for w in tokens if w.lower() not in stopwords_list]\n",
    "freq = FreqDist(stopped_tokens)\n",
    "freq.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.971911Z",
     "start_time": "2020-11-18T23:13:58.901Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "    \n",
    "    print(f\"- Tweet #{i}:\\n\")\n",
    "    print(corpus[i],'\\n')\n",
    "    tokens = word_tokenize(corpus[i])\n",
    "\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    \n",
    "    print(tokens,end='\\n\\n')\n",
    "    print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.972908Z",
     "start_time": "2020-11-18T23:13:58.904Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get FreqDist for Cleaned Text Data\n",
    "corpus[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Phases of Proprocessing/Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.973957Z",
     "start_time": "2020-11-18T23:13:58.909Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def clean_text(text,exclude_words=['until']):\n",
    "#     from nltk.corpus import stopwords\n",
    "#     import string\n",
    "#     from nltk import word_tokenize,regexp_tokenize\n",
    "#     ## tokenize text\n",
    "#     tokens = word_tokenize(text)\n",
    "#     # Get all the stop words in the English language\n",
    "#     stopwords_list = stopwords.words('english')\n",
    "#     stopwords_list += string.punctuation\n",
    "#     stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "#     return stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.975035Z",
     "start_time": "2020-11-18T23:13:58.913Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "    \n",
    "    print(f\"- Tweet #{i}:\\n\")\n",
    "    print(corpus[i],'\\n')\n",
    "    tokens = word_tokenize(corpus[i])\n",
    "\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    \n",
    "    print(tokens,end='\\n\\n')\n",
    "    print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best regexp resource and tester: https://regex101.com/\n",
    "\n",
    "    - Make sure to check \"Python\" under Flavor menu on left side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.976217Z",
     "start_time": "2020-11-18T23:13:58.918Z"
    }
   },
   "outputs": [],
   "source": [
    "text =  corpus[6615]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.977244Z",
     "start_time": "2020-11-18T23:13:58.922Z"
    }
   },
   "outputs": [],
   "source": [
    "text2=corpus[7347]\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.978297Z",
     "start_time": "2020-11-18T23:13:58.926Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize\n",
    "pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "regexp_tokenize(text,pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.979345Z",
     "start_time": "2020-11-18T23:13:58.929Z"
    }
   },
   "outputs": [],
   "source": [
    "print('[i] Word Tokenize:',end='\\n'+'---'*20+'\\n')\n",
    "print(word_tokenize(text))\n",
    "\n",
    "print('\\n[i] Regexp Tokenize:',end='\\n'+'---'*20+'\\n')\n",
    "print(regexp_tokenize(text,pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.980380Z",
     "start_time": "2020-11-18T23:13:58.933Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text,regex=True):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "\n",
    "    ## tokenize text\n",
    "    if regex:\n",
    "        pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "        tokens= regexp_tokenize(text,pattern)\n",
    "    else:\n",
    "        tokens = word_tokenize(text)\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    return stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.981481Z",
     "start_time": "2020-11-18T23:13:58.937Z"
    }
   },
   "outputs": [],
   "source": [
    "# @interact\n",
    "# def regexp_tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "#     print(f\"- Tweet #{i}:\\n\")\n",
    "#     print(corpus[i],'\\n')\n",
    "#     from nltk import regexp_tokenize\n",
    "#     pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "#     tokens= regexp_tokenize(corpus[i],pattern)\n",
    "\n",
    "#     # It is usually a good idea to lowercase all tokens during this step, as well\n",
    "#     stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "#     print(tokens,end='\\n\\n')\n",
    "#     return print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.982474Z",
     "start_time": "2020-11-18T23:13:58.942Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_urls(string): \n",
    "    return re.findall(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\",string)\n",
    "\n",
    "def find_hashtags(string):\n",
    "    return re.findall(r'\\#\\w*',string)\n",
    "\n",
    "def find_retweets(string):\n",
    "    return re.findall(r'RT [@]?\\w*:',string)\n",
    "\n",
    "def find_mentions(string):\n",
    "    return re.findall(r'\\@\\w*',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.983459Z",
     "start_time": "2020-11-18T23:13:58.946Z"
    }
   },
   "outputs": [],
   "source": [
    "find_urls(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.984538Z",
     "start_time": "2020-11-18T23:13:58.950Z"
    }
   },
   "outputs": [],
   "source": [
    "find_mentions(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.985530Z",
     "start_time": "2020-11-18T23:13:58.955Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('feet')) # foot\n",
    "print(lemmatizer.lemmatize('running')) # run [?!] Does not match expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.986551Z",
     "start_time": "2020-11-18T23:13:58.960Z"
    }
   },
   "outputs": [],
   "source": [
    "text_in =  corpus[6615]\n",
    "\n",
    "# # urls = find_urls(text)\n",
    "# def clean_text(text,regex=True):\n",
    "#     from nltk.corpus import stopwords\n",
    "#     import string\n",
    "#     from nltk import word_tokenize,regexp_tokenize\n",
    "\n",
    "#     ## tokenize text\n",
    "#     if regex:\n",
    "#         pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "#         tokens= regexp_tokenize(text,pattern)\n",
    "#     else:\n",
    "#         tokens = word_tokenize(text)\n",
    "#     # Get all the stop words in the English language\n",
    "#     stopwords_list = stopwords.words('english')\n",
    "#     stopwords_list += string.punctuation\n",
    "#     stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "#     return stopped_tokens\n",
    "\n",
    "def process_tweet(text,as_lemmas=False,as_tokens=True):\n",
    "#     text=text.copy()\n",
    "    for x in find_urls(text):\n",
    "        text = text.replace(x,'')\n",
    "        \n",
    "    for x in find_retweets(text):\n",
    "        text = text.replace(x,'')    \n",
    "        \n",
    "    for x in find_hashtags(text):\n",
    "        text = text.replace(x,'')    \n",
    "\n",
    "    if as_lemmas:\n",
    "        from nltk.stem.wordnet import WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = lemmatizer.lemmatize(text)\n",
    "    \n",
    "    if as_tokens:\n",
    "        text = clean_text(text)\n",
    "    \n",
    "    if len(text)==0:\n",
    "        text=''\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.987603Z",
     "start_time": "2020-11-18T23:13:58.965Z"
    }
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_processed_text(i=(0,len(corpus)-1)):\n",
    "    text_in = corpus[i]#.copy()\n",
    "    print(text_in)\n",
    "    text_out = process_tweet(text_in)\n",
    "    print(text_out)\n",
    "    text_out2 = process_tweet(text_in,as_lemmas=True)\n",
    "    print(text_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.988513Z",
     "start_time": "2020-11-18T23:13:58.969Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Potential Tasks: Classify Android vs iPhone tweets (from period where Android tweets still exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.989581Z",
     "start_time": "2020-11-18T23:13:58.978Z"
    }
   },
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df['created_at'])\n",
    "df\n",
    "\n",
    "df = df.set_index('datetime').sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.990656Z",
     "start_time": "2020-11-18T23:13:58.983Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text'].apply(process_tweet)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.991830Z",
     "start_time": "2020-11-18T23:13:58.989Z"
    }
   },
   "outputs": [],
   "source": [
    "android = df.groupby('source').get_group('Twitter for Android')\n",
    "android.index\n",
    "\n",
    "iphone = df.groupby('source').get_group('Twitter for iPhone').loc[:android.index[-1]]\n",
    "iphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.992835Z",
     "start_time": "2020-11-18T23:13:58.993Z"
    }
   },
   "outputs": [],
   "source": [
    "len(android), len(iphone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.993983Z",
     "start_time": "2020-11-18T23:13:58.997Z"
    }
   },
   "outputs": [],
   "source": [
    "df_corpus = pd.concat([iphone,android],axis=0)\n",
    "df_corpus['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count vectorization\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "    -  Used for multiple texts\n",
    "    \n",
    "    \n",
    "**_Term Frequency_** is calculated with the following formula:\n",
    "\n",
    "$$ \\text{Term Frequency}(t) = \\frac{\\text{number of times it appears in a document}} {\\text{total number of terms in the document}} $$ \n",
    "\n",
    "**_Inverse Document Frequency_** is calculated with the following formula:\n",
    "\n",
    "$$ IDF(t) = log_e(\\frac{\\text{Total Number of Documents}}{\\text{Number of Documents with it in it}})$$\n",
    "\n",
    "The **_TF-IDF_** value for a given word in a given document is just found by multiplying the two!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions/Topics \n",
    "- Next time: vectorization\n",
    "- Vs Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.995046Z",
     "start_time": "2020-11-18T23:13:59.005Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.996019Z",
     "start_time": "2020-11-18T23:13:59.009Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer.fit_transform(df_corpus['clean_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "232.713px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
