{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mod 4 Appendix: Deep NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bonus Office Hours S.G.\n",
    "- 11/18/20\n",
    "- online-ds-pt-041320"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You are not allowed to use any propriety functions from this notebook in your projects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discuss word Embeddings and their advantages\n",
    "- Training Word2Vec models\n",
    "- Using pretrained word embeddings\n",
    "\n",
    "\n",
    "- Create a Classification Model for true-trump (\"Twitter for Android\") vs trump-staffer(\"Twitter for iPhone - from period of time when android was still in use)\n",
    "\n",
    "    - Use lesson's W2Vec class in Sci-kit learn models\n",
    "    - Use LSTMs\n",
    "    - Use RNN/GRUs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Compare:\n",
    "    1.  Mean embeddings vs count/tfidf data with scikit learn.\n",
    "    \n",
    "\n",
    "<!-- ## References\n",
    "\n",
    "- My Work-in-Progress Capstone v2.0 Notebook:\n",
    "    - [GitHub Notebook Link](https://github.com/jirvingphd/capstone-project-using-trumps-tweets-to-predict-stock-market/blob/WIP/Capstone%20Restarted%2010-2020.ipynb) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP & Word Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Natural Language Processing_**, or **_NLP_**, is the study of how computers can interact with humans through the use of human language.  Although this is a field that is quite important to Data Scientists, it does not belong to Data Science alone.  NLP has been around for quite a while, and sits at the intersection of *Computer Science*, *Artificial Intelligence*, *Linguistics*, and *Information Theory*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-word-embeddings-online-ds-ft-100719/master/images/embeddings.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert words into a vector space\n",
    "    + Mathematical object\n",
    "- It's all about closeness\n",
    "    + Distributional Hypothesis: https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-word-embeddings-online-ds-ft-100719/master/images/vectors.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kaggle Tutorial:  https://www.kaggle.com/learn/embeddings\n",
    "- Google Embedding Crash Course: https://developers.google.com/machine-learning/crash-course/embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-using-word2vec-online-ds-ft-100719/master/images/training_data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the MLP to find the best weights (context) to map word-to-word\n",
    "- But since words close to another usually contain context, we're _really_ teaching it context in those weights\n",
    "- Gut check: similar contexted words can be exchanged\n",
    "    + EX: \"A fluffy **dog** is a great pet\" <--> \"A fluffy **cat** is a great pet\"\n",
    "\n",
    "- By training a text-generation model, we wind up with a lookup table where each word has its own vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-using-word2vec-online-ds-ft-100719/master/images/new_skip_gram_net_arch.png\">\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-using-word2vec-online-ds-ft-100719/master/images/new_word2vec_weight_matrix_lookup_table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe - Global Vectors for Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usually embeddings are hundreds of dimensions\n",
    "- Just use the word embeddings already learned from before!\n",
    "    + Unless very specific terminology, context will likely carry within language\n",
    "- Comparable to CNN transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Models - Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-understanding-recurrent-neural-networks-online-ds-ft-100719/master/images/unrolled.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-sequence-model-use-cases-online-ds-ft-100719/master/images/rnn.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs & GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GRU (Gated Recurrent Units (GRUs)\n",
    "    - Reset Gate\n",
    "    - Update Gate\n",
    "    \n",
    "- LSTM (Long Short Term Memory Cells)\n",
    "   - Input Gate\n",
    "   - Forget Gate\n",
    "   - Output Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-sequence-model-use-cases-online-ds-ft-100719/master/images/RNN-unrolled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word will have a vector of contexts: the embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity: Creating Word Embeddings with Trump's Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:46.540485Z",
     "start_time": "2020-11-18T23:48:46.538391Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -U fsds\n",
    "from fsds.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:48.008900Z",
     "start_time": "2020-11-18T23:48:46.544771Z"
    }
   },
   "outputs": [],
   "source": [
    "df = fs.datasets.load_nlp_finding_trump(read_csv_kwds={'parse_dates':['created_at'],\n",
    "                                                      'index_col':'created_at'})\n",
    "df.sort_index(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "\n",
    "- Two Part Word2Vec Tutorial  (linked from Learn)\n",
    "    - [Part 1: The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "    - [Part 2: Negative Sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `sentences`: dataset to train on\n",
    "- `size`: how big of a word vector do we want\n",
    "- `window`: how many words around the target word to train with\n",
    "- `min_count`: how many times the word shows up in corpus; we don't want words that are rarely used\n",
    "- `workers`: number of threads (individual task \"workers\")\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Let's assume we have our text corpus already tokenized and stored inside the variable 'data'--the regular text preprocessing steps still need to be handled before training a Word2Vec model!\n",
    "\n",
    "model = Word2Vec(data, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.train(data, total_examples=model.corpus_count)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:48.013061Z",
     "start_time": "2020-11-18T23:48:48.010669Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, TweetTokenizer\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:48.910211Z",
     "start_time": "2020-11-18T23:48:48.015031Z"
    }
   },
   "outputs": [],
   "source": [
    "## TRAINING WORD2VEC FROM FULL DF NOT JUST TARGETS\n",
    "data_lower = df['text'].map(lambda x: simple_preprocess(x,deacc=True,max_len=100))\n",
    "data_lower[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:48.914324Z",
     "start_time": "2020-11-18T23:48:48.912255Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:50.709412Z",
     "start_time": "2020-11-18T23:48:48.915673Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(data_lower,size=100,window=5,min_count=3,workers=4)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:52.126856Z",
     "start_time": "2020-11-18T23:48:50.711124Z"
    }
   },
   "outputs": [],
   "source": [
    "model.train(data_lower, total_examples=model.corpus_count,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:52.132683Z",
     "start_time": "2020-11-18T23:48:52.128370Z"
    }
   },
   "outputs": [],
   "source": [
    "wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:52.137684Z",
     "start_time": "2020-11-18T23:48:52.135851Z"
    }
   },
   "outputs": [],
   "source": [
    "# help(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:52.143990Z",
     "start_time": "2020-11-18T23:48:52.139924Z"
    }
   },
   "outputs": [],
   "source": [
    "wv['republican']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:52.152072Z",
     "start_time": "2020-11-18T23:48:52.145479Z"
    }
   },
   "outputs": [],
   "source": [
    "wv.most_similar('republican')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:52.158633Z",
     "start_time": "2020-11-18T23:48:52.154013Z"
    }
   },
   "outputs": [],
   "source": [
    "wv.most_similar(negative=['democrat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "## For initializing model\n",
    "sentences=None,\n",
    "    size=100,\n",
    "    alpha=0.025,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    max_vocab_size=None,\n",
    "    sample=0.001,\n",
    "    seed=1,\n",
    "    workers=3,\n",
    "    min_alpha=0.0001,\n",
    "    sg=0,\n",
    "    hs=0,\n",
    "    negative=5,\n",
    "    cbow_mean=1,\n",
    "    hashfxn=<built-in function hash>,\n",
    "    iter=5,\n",
    "    null_word=0,\n",
    "    trim_rule=None,\n",
    "    sorted_vocab=1,\n",
    "    batch_words=10000,\n",
    "    compute_loss=False,\n",
    "    callbacks=(),\n",
    "    \n",
    "    \n",
    "## For training \n",
    "    sentences,\n",
    "    total_examples=None,\n",
    "    total_words=None,\n",
    "    epochs=None,\n",
    "    start_alpha=None,\n",
    "    end_alpha=None,\n",
    "    word_count=0,\n",
    "    queue_factor=2,\n",
    "    report_delay=1.0,\n",
    "    compute_loss=False,\n",
    "    callbacks=(),\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:52.165652Z",
     "start_time": "2020-11-18T23:48:52.160497Z"
    }
   },
   "outputs": [],
   "source": [
    "# ### USING WORD VECTOR MATH TO GET A FEEL FOR QUALITY OF MODE\n",
    "def word_math(wv,pos_words=['hillary'],neg_words=['bill'],\n",
    "              verbose=True,return_vec=False):\n",
    "    if isinstance(pos_words,str):\n",
    "        pos_words=[pos_words]\n",
    "    if isinstance(neg_words,str):\n",
    "        neg_words=[neg_words]\n",
    "\n",
    "\n",
    "    pos_eqn = '+'.join(pos_words)\n",
    "    neg_eqn = '-'.join(neg_words)\n",
    "\n",
    "    print('---'*15)    \n",
    "    print(f\"[i] Result for:\\t{pos_eqn}{' - '+neg_eqn if len(neg_eqn)>0 else ' '}\")\n",
    "    print('---'*15)\n",
    "\n",
    "    answer = wv.most_similar(positive=pos_words,negative=neg_words)\n",
    "    \n",
    "    if verbose:\n",
    "          [print(f\"- {ans[0]} ({round(ans[1],3)})\") for ans in answer]\n",
    "          print('---'*15,'\\n\\n')\n",
    "\n",
    "    if return_vec: \n",
    "          return answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:52.179755Z",
     "start_time": "2020-11-18T23:48:52.167662Z"
    }
   },
   "outputs": [],
   "source": [
    "equation_list=[(['america','crime'],[]),\n",
    "               \n",
    "               (['democrats','russia'],[]),\n",
    "               (['republican'],['honor']),\n",
    "               (['man','power'],[]),\n",
    "               (['russia','honor'],[]),\n",
    "              (['china','tariff'])]\n",
    "\n",
    "for eqn in equation_list:\n",
    "#     print('\\n\\n')\n",
    "    word_math(wv,*eqn)\n",
    "#     word_math(wv2,*eqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pre-Trained Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:52.185903Z",
     "start_time": "2020-11-18T23:48:52.181923Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "folder = '/Users/jamesirving/Datasets/'#glove.twitter.27B/'\n",
    "# print(os.listdir(folder))\n",
    "glove_file = folder+'glove.6B/glove.6B.50d.txt'#'glove.twitter.27B.50d.txt'\n",
    "glove_twitter_file = folder+'glove.twitter.27B/glove.twitter.27B.50d.txt'\n",
    "print(glove_file)\n",
    "print(glove_twitter_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping only the vectors needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:52.190350Z",
     "start_time": "2020-11-18T23:48:52.188115Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## This line of code for getting all words bugs me\n",
    "# total_vocabulary = set(word for tweet in data_lower for word in tweet)\n",
    "# len(total_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:52.196003Z",
     "start_time": "2020-11-18T23:48:52.193157Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# glove = {}\n",
    "# with open(glove_file,'rb') as f:#'glove.6B.50d.txt', 'rb') as f:\n",
    "#     for line in f:\n",
    "#         parts = line.split()\n",
    "#         word = parts[0].decode('utf-8')\n",
    "#         if word in total_vocabulary:\n",
    "#             vector = np.array(parts[1:], dtype=np.float32)\n",
    "#             glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:52.200984Z",
     "start_time": "2020-11-18T23:48:52.198645Z"
    }
   },
   "outputs": [],
   "source": [
    "# glove['republican']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Glove to Word2Vec format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting glove into w2vec format:\n",
    "    - https://radimrehurek.com/gensim/scripts/glove2word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:48:52.207806Z",
     "start_time": "2020-11-18T23:48:52.203328Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_folder = folder+'glove.twitter.27B'\n",
    "os.listdir(glove_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:55:26.696703Z",
     "start_time": "2020-11-18T23:54:29.485411Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath(glove_twitter_file)\n",
    "tmp_file = get_tmpfile(glove_folder+'glove_to_w2vec.txt')\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "model_glove = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:55:26.702411Z",
     "start_time": "2020-11-18T23:55:26.698528Z"
    }
   },
   "outputs": [],
   "source": [
    "model_glove.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:55:27.448596Z",
     "start_time": "2020-11-18T23:55:26.705425Z"
    }
   },
   "outputs": [],
   "source": [
    "## Using pre-trained embeddings for math\n",
    "equation_list=[(['america','crime'],[]),\n",
    "               \n",
    "               (['democrats','russia'],[]),\n",
    "               (['republican'],['honor']),\n",
    "               (['man','power'],[]),\n",
    "               (['russia','honor'],[]),\n",
    "              (['china','tariff'])]\n",
    "\n",
    "for eqn in equation_list:\n",
    "#     print('\\n\\n')\n",
    "    word_math(model_glove,*eqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification - Finding Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:49:43.491062Z",
     "start_time": "2020-11-18T23:49:43.468877Z"
    }
   },
   "outputs": [],
   "source": [
    "## Getting time period with android tweets\n",
    "droid_ts = df[df['source'] == 'Twitter for Android'].index\n",
    "find_trump = df.loc[droid_ts[0]:droid_ts[-1]].copy()\n",
    "\n",
    "## Getting only original-text (not retweets)\n",
    "find_trump = find_trump[find_trump['is_retweet']==False]\n",
    "find_trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:49:43.508707Z",
     "start_time": "2020-11-18T23:49:43.497936Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_trump(x):\n",
    "    return 'Trump' if x =='Twitter for Android' else 'Not Trump'\n",
    "find_trump['is_trump'] = find_trump['source'].map(is_trump)\n",
    "find_trump['is_trump'].value_counts(dropna=False,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:49:43.538084Z",
     "start_time": "2020-11-18T23:49:43.511596Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_map = {'Not Trump':0,'Trump':1}\n",
    "\n",
    "find_trump['target'] = find_trump['is_trump'].map(target_map)\n",
    "find_trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:49:43.553132Z",
     "start_time": "2020-11-18T23:49:43.540171Z"
    }
   },
   "outputs": [],
   "source": [
    "df = find_trump[['text','is_trump','target']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Embeddings in Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layers\n",
    "You should make note of a couple caveats that come with using embedding layers in your neural network -- namely:\n",
    "\n",
    "* The embedding layer must always be the first layer of the network, meaning that it should immediately follow the `Input()` layer \n",
    "* All words in the text should be integer-encoded, with each unique word encoded as it's own unique integer  \n",
    "* The size of the embedding layer must always be greater than the total vocabulary size of the dataset! The first parameter denotes the vocabulary size, while the second denotes the size of the actual word vectors\n",
    "* The size of the sequences passed in as data must be set when creating the layer (all data will be converted to padded sequences of the same size during the preprocessing step) \n",
    "\n",
    "\n",
    "[Keras Documentation for Embedding Layers](https://keras.io/layers/embeddings/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Embedding Layers in ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:49:43.558335Z",
     "start_time": "2020-11-18T23:49:43.554861Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:49:43.570111Z",
     "start_time": "2020-11-18T23:49:43.560001Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:52:55.333464Z",
     "start_time": "2020-11-18T23:52:55.330816Z"
    }
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append(\"../\")\n",
    "import keras_gridsearch as kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:52:55.522130Z",
     "start_time": "2020-11-18T23:52:55.516651Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import text,sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn import metrics\n",
    "\n",
    "X = df['text']\n",
    "y_t = to_categorical(df['target'])\n",
    "y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:52:55.746396Z",
     "start_time": "2020-11-18T23:52:55.707750Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_WORDS = 25000\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=MAX_WORDS)\n",
    "\n",
    "tokenizer.fit_on_texts(X) #df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(X) #df['text'])\n",
    "\n",
    "X_t = sequence.pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:52:55.922428Z",
     "start_time": "2020-11-18T23:52:55.918273Z"
    }
   },
   "outputs": [],
   "source": [
    "X_t[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:52:56.209327Z",
     "start_time": "2020-11-18T23:52:56.203671Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(X_t,y_t,random_state=123) \n",
    "X_train.shape,y_test.shape\n",
    "# pd.Series(y_test).value_counts(normalize=True)\n",
    "y_test.shape\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:53:02.176663Z",
     "start_time": "2020-11-18T23:52:57.289869Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128 #where codealong get this?\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(MAX_WORDS, EMBEDDING_SIZE))\n",
    "model.add(LSTM(25,return_sequences=True))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',#'categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_hat_test = model.predict_classes(X_test)\n",
    "# print(pd.Series(y_hat_test).value_counts())\n",
    "kg.evaluate_model(y_test,y_hat_test,history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Glove In Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:55:33.587626Z",
     "start_time": "2020-11-18T23:55:33.583562Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_layer = model_glove.get_keras_embedding()\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:56:04.891984Z",
     "start_time": "2020-11-18T23:56:01.925748Z"
    }
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(25,return_sequences=True))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',#'categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_hat_test = model.predict_classes(X_test)\n",
    "# print(pd.Series(y_hat_test).value_counts())\n",
    "kg.evaluate_model(y_test,y_hat_test,history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Approach to Loading in Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:52:56.565734Z",
     "start_time": "2020-11-18T23:52:56.563143Z"
    }
   },
   "outputs": [],
   "source": [
    "# EMBEDDING_SIZE = 128 #where codealong get this?\n",
    "\n",
    "# embedding_matrix = np.zeros((len(total_vocabulary) + 1, EMBEDDING_SIZE))\n",
    "# for word, i in enumerate(total_vocabulary):#.items():\n",
    "#     embedding_vector = glove.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "        \n",
    "# embedding_layer = Embedding(len(total_vocabulary) + 1,\n",
    "#                             EMBEDDING_SIZE,\n",
    "#                             weights=[embedding_matrix],\n",
    "#                             input_length=MAX_SEQUENCE_LENGTH,\n",
    "#                             trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:18:01.708621Z",
     "start_time": "2020-02-17T00:18:00.772373Z"
    }
   },
   "source": [
    "## RNN or GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:53:25.534334Z",
     "start_time": "2020-11-18T23:53:25.196975Z"
    }
   },
   "outputs": [],
   "source": [
    "## GRU Model\n",
    "from keras import models, layers, optimizers, regularizers\n",
    "modelG = models.Sequential()\n",
    "\n",
    "## Get and add embedding_layer\n",
    "# embedding_layer = ji.make_keras_embedding_layer(wv, X_train)\n",
    "modelG.add(Embedding(MAX_WORDS, EMBEDDING_SIZE))\n",
    "\n",
    "# modelG.add(layers.SpatialDropout1D(0.5))\n",
    "# modelG.add(layers.Bidirectional(layers.GRU(units=100, dropout=0.5, recurrent_dropout=0.2,return_sequences=True)))\n",
    "modelG.add(layers.Bidirectional(layers.GRU(units=100, dropout=0.5, recurrent_dropout=0.2)))\n",
    "modelG.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "modelG.compile(loss='categorical_crossentropy',optimizer=\"adam\",metrics=['acc'])#,'val_acc'])#, callbacks=callbacks)\n",
    "modelG.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:53:39.994032Z",
     "start_time": "2020-11-18T23:53:26.093134Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "history = modelG.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_hat_test = modelG.predict_classes(X_test)\n",
    "kg.evaluate_model(y_test,y_hat_test,history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Embeddings in Classification Models - sci-kit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embeddings can be used in Artificial Neural Networks as an input Embedding Layer\n",
    "- Embeddings can be used in sci-kit learn models by taking the mean vector of a text/document and using the mean vector as the input into the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Mean Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.948402Z",
     "start_time": "2020-11-18T23:13:58.810Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "\n",
    "y = pd.get_dummies(df['source'],drop_first=True).values\n",
    "X = df['text'].str.lower().map(word_tokenize)\n",
    "\n",
    "X_idx = list(range(len(X)))\n",
    "train_idx,test_idx = train_test_split(X_idx,random_state=123)\n",
    "\n",
    "X[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.949497Z",
     "start_time": "2020-11-18T23:13:58.815Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split_idx(X, y, train_idx,test_idx):\n",
    "    # try count vectorized first\n",
    "    X_train = X[train_idx].copy()\n",
    "    y_train = y[train_idx].copy()\n",
    "    X_test = X[train_idx].copy()\n",
    "    y_test = y[train_idx].copy()\n",
    "    return X_train, X_test,y_train, y_test\n",
    "\n",
    "X_train, X_test,y_train, y_test = train_test_split_idx(X,y,train_idx,test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.950506Z",
     "start_time": "2020-11-18T23:13:58.819Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['combined_text'] = df['headline'] + ' ' + df['short_description']\n",
    "# data = df['combined_text'].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.951538Z",
     "start_time": "2020-11-18T23:13:58.823Z"
    }
   },
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # Takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # it can't be used in a scikit-learn pipeline  \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.952463Z",
     "start_time": "2020-11-18T23:13:58.827Z"
    }
   },
   "outputs": [],
   "source": [
    "# target = df['source']\n",
    "# data = df['text'].map(word_tokenize)\n",
    "# data_lower = list(map(lambda x: [w.lower() for w in x],data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.953402Z",
     "start_time": "2020-11-18T23:13:58.830Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])\n",
    "\n",
    "models = [('Random Forest', rf),\n",
    "          ('Support Vector Machine', svc),\n",
    "          ('Logistic Regression', lr)]\n",
    "# models = {'Random Forest':RandomForestClassifier(n_estimators=100, verbose=True),\n",
    "#           'SVC':SVC(),'lr':LogisticRegression()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.954590Z",
     "start_time": "2020-11-18T23:13:58.834Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = [(name, cross_val_score(model, X_train, y_train, cv=2).mean()) for name, model, in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T23:15:07.955778Z",
     "start_time": "2020-11-18T23:13:58.838Z"
    }
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "410.688px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
