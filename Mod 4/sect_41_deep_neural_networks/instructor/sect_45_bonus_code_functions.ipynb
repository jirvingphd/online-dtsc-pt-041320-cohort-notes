{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF 02-11-2020 STUDY GROUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameter Tuning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Source: https://chrisalbon.com/deep_learning/keras/tuning_neural_network_hyperparameters/\n",
    "<br><br>\n",
    "\n",
    "- To use `GridSearchCV` or other similar functions in scikit-learn with a Keras neural network, we need to wrap our keras model in `keras.wrappers.scikit_learn`'s `KerasClassifier` and `KerasRegressor`.\n",
    "1. To do this, we need to write a build function(`build_fn`) that creates our model such as `create_model`.\n",
    "    - This function must accept whatever parameters you wish to tune. \n",
    "    - It also must have a default argument for each parameter.\n",
    "    - This function must Return the model (and only the model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T16:52:34.011618Z",
     "start_time": "2020-02-11T16:52:34.007640Z"
    }
   },
   "outputs": [],
   "source": [
    "n_units=(64,32,10)\n",
    "activations=('relu','relu','softmax')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "## Define the build function\n",
    "def create_model(n_units=(50,25,7), activation='relu',final_activation='softmax',\n",
    "                optimizer='adam'):\n",
    "    \n",
    "    ## Pro tip:save the local variables now so you can print out the parameters used to create the model.\n",
    "    params_used = locals()\n",
    "    print('Parameters for model:\\n',params_used)\n",
    "    \n",
    "   \n",
    "    from keras.models import Sequential\n",
    "    from keras import layers\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(layers.Dense(n_units[0], activation=activation, input_shape=(2000,)))\n",
    "    model.add(layers.Dense(n_units[1], activation=activation))\n",
    "    model.add(layers.Dense(n_units[2], activation=final_activation))\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    display(model.summary())\n",
    "    return model \n",
    "```    \n",
    "\n",
    "2. We then create out model using the Keras wrapper:\n",
    "\n",
    "```python\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "neural_network =  KerasClassifier(build_fn=create_model,verbose=1)\n",
    "```\n",
    "\n",
    "3. Now, set up the hyperparameter space for grid search. (Remember, your `create_model` function must accept the parameter you want to tune)\n",
    "\n",
    "```python\n",
    "params_to_test = {'n_units':[(50,25,7),(100,50,7)],\n",
    "                  'optimizer':['adam','rmsprop','adadelta'],\n",
    "                  'activation':['linear','relu','tanh'],\n",
    "                  'final_activation':['softmax']}\n",
    "```\n",
    "\n",
    "4. Now instantiate your GridSearch function\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(estimator=neural_network,param_grid=params_to_test)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "best_params = grid_result.best_params_\n",
    "```\n",
    "5. And thats it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T17:01:34.173140Z",
     "start_time": "2020-02-11T17:01:34.124492Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T17:02:40.083379Z",
     "start_time": "2020-02-11T17:02:40.062684Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T16:52:34.336107Z",
     "start_time": "2020-02-11T16:52:13.048Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C3U8KA5IPuXV"
   },
   "source": [
    "\n",
    "### Tools & Applications introduced\n",
    "- **Numpy Functions**\n",
    "    - To unrow a 3-D image ( row, cols, RGB):\n",
    "        - `image_array.shape # returns (790, 64,64,3) - 790 imgs, size = 64,64, RGB=3\n",
    "        - `img_unrow = img.reshape(len(image_array), -1).T`\n",
    "            - -1 takes care of the remaining dims\n",
    "    - To increase the 'rank' of a vector (the first dimension) \n",
    "        - `np.reshape(vector, (1,len(image_array))`\n",
    "- **Keras**\n",
    "    - `from keras import models, layers, optimizers`\n",
    "    - `keras.preprocessing.image` \n",
    "    - `keras.preprocessing.text`\n",
    "- `from matplotlib.pyplot import imread, imshow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOW TO: Custom Scoring Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Why would you do this?**\n",
    "    1. You may want to use a metric that isn't available in sklearn. \n",
    "        - In the included `my_custom_scorer` function, I take the accuracy of each class's predictions from the diagonal of a normalized confusion matrix. \n",
    "        - I then calculate the mean of those 3 class accuracies, which is the `score` that is returned to the gridsearch. \n",
    "        \n",
    "    2. You may want to add a printout or display to the scoring function so you can see the results as the search is going.\n",
    "<br><br>\n",
    "2. **How do you do write your own?**\n",
    "    1. Define your custom scoring function.\n",
    "        - It must accept `y_true`,`y_pred`\n",
    "        - It must return a value to maximize. (like accuracy)\n",
    "    2. You can add print or display commands to have the scoring function report the current results as the gridsearch is still going.\n",
    "        - If you combine this with the example `create_model` function above that includes the `vars=locals(); print(vars)` command, then gridsearch will display:\n",
    "            1. the parameters of each model (each time the `create_model` function is called.\n",
    "            2. The score of each model, including a confusion matrix figure (each time it calls `my_custom_scorer`).\n",
    "        \n",
    "```python\n",
    "def my_custom_scorer(y_true,y_pred):\n",
    "    \"\"\"My custom score function to use with sklearn's GridSearchCV\n",
    "    Maximizes the average accuracy per class using a normalized confusion matrix\"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import numpy as np\n",
    "    import functions_combined_BEST as ji    \n",
    "\n",
    "    ## Flatten one-hot encoded target columns into 1 column for sklearn functions\n",
    "    if y_true.ndim>1 or y_pred.ndim>1:\n",
    "        \n",
    "        ## reduce dimensions of y_train and y_test\n",
    "        if y_true.ndim>1:            \n",
    "            y_true = y_true.argmax(axis=1)\n",
    "        \n",
    "        if y_pred.ndim>1:\n",
    "            y_pred = y_pred.argmax(axis=1)\n",
    "\n",
    "    \n",
    "     # Get confusion matrx\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Normalize confusion matrix\n",
    "    cm_norm = cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "\n",
    "    ## Get diagonals for class accuracy\n",
    "    diag = cm_norm.diagonal()\n",
    "    \n",
    "    # Get the mean of the diagonal values\n",
    "    score = np.mean(diag)\n",
    "    \n",
    "    ## Display Results for the User\n",
    "    print(f'Mean Class Accuracy = {score}')\n",
    "    print(f'Class Accuracy Values:')\n",
    "    print(diag)    \n",
    "\n",
    "    ## Plot the confusion matrix.\n",
    "    ji.plot_confusion_matrix(cm,normalize=True)\n",
    "\n",
    "    # return the score \n",
    "    return score\n",
    "```        \n",
    "        \n",
    "        \n",
    "3. **How do you use it?**\n",
    "    - When instantiating GridSearchCV pass your function as the `scoring=` parameter, wrapped in the  `sklearn.metrics.make_scorer` function.\n",
    "  \n",
    "\n",
    "```python\n",
    "## Using custom scoring function\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "grid = GridSearchCV(estimator=neural_network, \n",
    "                    param_grid=params_to_test,\n",
    "                   scoring=make_scorer(my_custom_scorer))\n",
    "                    \n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T16:52:34.336912Z",
     "start_time": "2020-02-11T16:52:13.055Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_secret_password(file='/Users/jamesirving/.secret/gmail.json'):\n",
    "    with open(file) as file:\n",
    "        import json\n",
    "        gmail = json.loads(file.read())\n",
    "    # email_notification()\n",
    "    print(gmail.keys())\n",
    "    return gmail\n",
    "\n",
    "def email_notification(password_obj=None,subject='GridSearch Finished',msg='The GridSearch is now complete.'):\n",
    "    \"\"\"Sends email notification from gmail account using previously encrypyted password  object (an instance\n",
    "    of EncrypytedPassword). \n",
    "    Args:\n",
    "        password_obj (EncryptedPassword object): EncryptedPassword object with username/password.\n",
    "        subject (str):Text for subject line.\n",
    "        msg (str): Text for body of email. \n",
    "\n",
    "    Returns:\n",
    "        Prints `Email sent!` if email successful. \n",
    "    \"\"\"\n",
    "    ## Display instructions if no password_obj \n",
    "    if password_obj is None:\n",
    "        print('Must pass an EncrypytedPassword object.')\n",
    "        print('>> pwd_obj = EncryptedPassword(username=\"my_username\",password=\"my_password\")')\n",
    "        print('>> send_email(encrypted_password_obj=pwd_obj)')\n",
    "        raise Exception('Must pass an EncryptedPassword.')\n",
    "    if isinstance(password_obj,dict):\n",
    "        gmail_user = password_obj['username']\n",
    "        gmail_password = password_obj['password']\n",
    "    else:\n",
    "        \n",
    "        ## Get username and password from password_obj\n",
    "        gmail_user = password_obj.username\n",
    "        gmail_password = password_obj._password_\n",
    "        \n",
    "    \n",
    "    # import required packages\n",
    "    import smtplib\n",
    "    from email.mime.multipart import MIMEMultipart\n",
    "    from email.mime.text import MIMEText\n",
    "    from email import encoders\n",
    "    \n",
    "\n",
    "    ## WRITE EMAIL\n",
    "    message = MIMEMultipart()\n",
    "    message['Subject'] =subject\n",
    "    message['To'] = gmail_user\n",
    "    message['From'] = gmail_user\n",
    "    body = msg\n",
    "    message.attach(MIMEText(body,'plain'))\n",
    "    text_message = message.as_string()\n",
    "\n",
    "\n",
    "    # Send email request\n",
    "    try:\n",
    "        with  smtplib.SMTP_SSL('smtp.gmail.com',465) as server:\n",
    "            \n",
    "            server.login(gmail_user,gmail_password)\n",
    "            server.sendmail(gmail_user,gmail_user, text_message)\n",
    "            server.close()\n",
    "            print('Email sent!')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Something went wrong')\n",
    "        \n",
    "        \n",
    "\n",
    "# gmail = get_secret_password()\n",
    "# email_notification(gmail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T16:52:34.337697Z",
     "start_time": "2020-02-11T16:52:13.058Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_custom_scorer(y_true,y_pred):\n",
    "    \"\"\"My custom score function to use with sklearn's GridSearchCV\n",
    "    Maximizes the average accuracy per class using a normalized confusion matrix\n",
    "    [i] Note: To use my_custom_scorer in GridSearch:\n",
    "    >> from sklearn.metrics import make_scorer\n",
    "    >> grid = GridSearch(estimator, parameter_grid)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import make_scorer,confusion_matrix\n",
    "    import numpy as np\n",
    "    import functions_combined_BEST as ji    \n",
    "\n",
    "    # set labels for confusion matrix\n",
    "    labels = ['Decrease','No Change', 'Increase']\n",
    "\n",
    "    \n",
    "    ## If y_true is a multi-column one-hotted target\n",
    "    if y_true.ndim>1 or y_pred.ndim>1:\n",
    "\n",
    "        ## reduce dimensions of y_train and y_test\n",
    "        if y_true.ndim>1:            \n",
    "            y_true = y_true.argmax(axis=1)\n",
    "            \n",
    "        if y_pred.ndim>1:\n",
    "            y_pred = y_pred.argmax(axis=1)\n",
    "\n",
    "    \n",
    "    ## Get confusion matrx\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    ## Normalize confusion matrix\n",
    "    cm_norm = cm.astype('float')/cm.sum(axis=1)[:,np.newaxis]\n",
    "\n",
    "    ## Get diagonals for class accuracy\n",
    "    diag = cm_norm.diagonal()\n",
    "    score = np.mean(diag)\n",
    "    \n",
    "    \n",
    "    ## Display results for user\n",
    "    print(f'Mean Class Accuracy = {score}')\n",
    "    print(f'Class Accuracy Values:')\n",
    "    print(diag)    \n",
    "\n",
    "    ## Plot confusion matrix\n",
    "    ji.plot_confusion_matrix(cm,normalize=True,classes=labels);\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y0QuBXKwRlbc"
   },
   "source": [
    "## Overview - Network Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y0QuBXKwRlbc"
   },
   "source": [
    "### Normalization\n",
    "- Normalizing to a consistent scale (typically 0 to 1) improves performance, but also ensures the process will converge to a stable solution. \n",
    "\n",
    "- Methods:\n",
    "    - Z-Score (subtracting mean, normalize by standard deviation)\n",
    "    \n",
    "#### Reference Links\n",
    "- https://www.coursera.org/lecture/deep-neural-network/normalizing-inputs-lXv6U\n",
    "\n",
    "### Changing Initial Parameters\n",
    "- The more input features into layer $l$, the smaller we want each weight $w_i$ to be.\n",
    "- Rule of thumb:\n",
    "    - $Var(w_i) = 1/n$ or $2/n$\n",
    "- A common initilization strategy for the relu activation functions is:\n",
    "\n",
    "    * $w^{[l]}$ `= np.random.randn(shape)*np.sqrt(2/n_(l-1))`\n",
    "    \n",
    "## Optimization:\n",
    "Alternatives to gradient descent that do not oscillate as much as g.d.:\n",
    "### Gradient Descent with Momentum:\n",
    "- Comutes an exponentially weighted average of the gradients to use.\n",
    "    - will dampen oscillations and improve performance.\n",
    "- How to:\n",
    "    -  Calculate current batch's moving averages for the derivatives of $W$ and$b$\n",
    "        - Compute $V_{dw} = \\beta V_{dw} + (1-\\beta)dW$\n",
    "        - $V_{db} = \\beta V_{db} + (1-\\beta)db$ \n",
    "        - So updated terms become\n",
    "            - $W:= W- \\alpha Vdw$\n",
    "            -$b:= b- \\alpha Vdb$\n",
    "    -  Generally, $\\beta=0.9$ is a good hyperparameter value.\n",
    "    \n",
    "### RMSprop\n",
    "- \"Root mean square\" prop\n",
    "- Slow down learning in one direction and speed it up in another.\n",
    "    - In the direction where we want to learn fast, the corresponding S will be small, so dividing by a small number. \n",
    "    - In the direction where we will want to learn slow, the corresponding S will be relatively large, and updates will be smaller. \n",
    "- How to:\n",
    "    - On each iteration, use exponentially weighted average again:\n",
    "        - exponentially weighted average of the squares of the derivatives\n",
    "        - $S_{dw} = \\beta S_{dw} + (1-\\beta)dW^2$\n",
    "        - $S_{db} = \\beta S_{dw} + (1-\\beta)db^2$\n",
    "        - So that:\n",
    "            - $W:= W- \\alpha \\dfrac{dw}{\\sqrt{S_{dw}}}$\n",
    "            - $b:= b- \\alpha \\dfrac{db}{\\sqrt{S_{db}}}$\n",
    "    - Often, add small $\\epsilon$ in the denominator to make sure that you don't end up dividing by 0.\n",
    "\n",
    "\n",
    "### Adam Optimization Algorithm\n",
    "- Adaptive Moment Estimation - essentially combines both methods above.\n",
    "- Works very well in most situations.\n",
    "- How to: \n",
    "    - Initialize: $V_{dw}=0, S_{dw}=0, V_{db}=0, S_{db}=0$.\n",
    "    - For each teration: compute $dW, db$ using the current mini-batch.\n",
    "        -  $V_{dw} = \\beta_1 V_{dw} + (1-\\beta_1)dW$, $V_{db} = \\beta_1 V_{db} + (1-\\beta_1)db$ \n",
    "        -  $S_{dw} = \\beta_2 S_{dw} + (1-\\beta_2)dW^2$, $S_{db} = \\beta_2 S_{db} + (1-\\beta_2)db^2$ \n",
    "        \n",
    "- As with  momentum and then RMSprop. We need to perform a correction! This is sometimes also done in RSMprop, but definitely here too.\n",
    "    - $V^{corr}_{dw}= \\dfrac{V_{dw}}{1-\\beta_1^t}$, $V^{corr}_{db}= \\dfrac{V_{db}}{1-\\beta_1^t}$\n",
    "\n",
    "    - $S^{corr}_{dw}= \\dfrac{S_{dw}}{1-\\beta_2^t}$, $S^{corr}_{db}= \\dfrac{S_{db}}{1-\\beta_2^t}$\n",
    "\n",
    "    - $W:= W- \\alpha \\dfrac{V^{corr}_{dw}}{\\sqrt{S^{corr}_{dw}+\\epsilon}}$ and\n",
    "\n",
    "    - $b:= b- \\alpha \\dfrac{V^{corr}_{db}}{\\sqrt{S^{corr}_{db}+\\epsilon}}$ \n",
    "\n",
    "\n",
    "### Learning Rate Decay\n",
    "- Learning rate decreases across epochs.\n",
    "    - $\\alpha = \\dfrac{1}{1+\\text{decay_rate * epoch_nb}}* \\alpha_0$\n",
    "\n",
    "- other methods:\n",
    "    - $\\alpha = 0.97 ^{\\text{epoch_nb}}* \\alpha_0$ (or exponential decay)<br>OR:\n",
    "    - $\\alpha = \\dfrac{k}{\\sqrt{\\text{epoch_nb}}}* \\alpha_0$<br> OR:\n",
    "    - Manual decay.\n",
    "    \n",
    "    \n",
    "    \n",
    "### HYPERPARAMETER TUNING:\n",
    "Most important:\n",
    "- $\\alpha$\n",
    "\n",
    "Important next:\n",
    "- $\\beta$ (momentum)\n",
    "- Number of hidden units\n",
    "- mini-batch-size\n",
    "\n",
    "Finally:\n",
    "- Number of layers\n",
    "- Learning rate decay\n",
    "\n",
    "Almost never tuned:\n",
    "- $\\beta_1$, $\\beta_2$, $\\epsilon$ (Adam)\n",
    "\n",
    "- Tip: Don't use a grid, because hard to say in advance which hyperparameters will be important.\n",
    "\n",
    "\n",
    "### OPTIMIZAITON REFS:\n",
    "- https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "- https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "- https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "- https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network https://www.springboard.com/blog/free-public-data-sets-data-science-project/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKw2nWdMDKR6"
   },
   "source": [
    "## Section 42: Network Regularlization & Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbzweQKTDPTl"
   },
   "source": [
    "## Overview - Regularization\n",
    "- Bias vs variance trade-off\n",
    "- Using test, train, and vali splits. \n",
    "- Prevent overfitting by adding regularization methods (L1, L2, dropout)\n",
    "- Optimizing and training time reduction by normalizing inputs\n",
    "    - Normalizing inputs can drasticaly decrease computation time, and prevent vanishing/exploding graidents. \n",
    "    \n",
    "### Hyperparameters to Tune\n",
    "- Number of hidden units\n",
    "- Number of layers\n",
    "- Learning rate ( $\\alpha$)\n",
    "- Activation function\n",
    "\n",
    "### Training, Validation, and Test Sets\n",
    "- The fact that there are so many hyperparameters to tune calls for a formalized and unbiased approach to testing/training sets.\n",
    "- We will use 3 sets when running, selecting, and validating a model:\n",
    "    - Training set: for training the alogrithm\n",
    "    - Validation set: to decide which model will be the final one after parameter tuning\n",
    "    - Testing set: after choosing final  the final model, use the test set for an inbiased estimate of performance. \n",
    "- Set sizes:\n",
    "    - With big data, your dev and test sets don't necessarily need to be 20-30% of all the data. \n",
    "    - You can choose test and hold-out sets that are of size 1-5%. \n",
    "        - eg. 96% train, 2% hold-out, 2% test set. \n",
    "    - It is **VERY IMPORTANT** to make sure holdout and test sample come from the same distribution: eg. same resolution of santa pictures. \n",
    "    \n",
    "### Bias vs Variance \n",
    "- A model with high bias may result in underfitting.\n",
    "    - <img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-42-02-tuning-neural-networks-with-regularization-online-ds-ft-021119/master/figures/underfitting.png\" width=200>\n",
    "- A model with high variance may result in overfitting. \n",
    "    - <img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-42-02-tuning-neural-networks-with-regularization-online-ds-ft-021119/master/figures/overfitting.png\" width=200>\n",
    "\n",
    "- In deep learning, there is less of a bias-variance trad-off vs simpler models. \n",
    "\n",
    "**Rules of thumb re: bias/variance trade-off:**\n",
    "\n",
    "| High Bias? (training performance) | high variance? (validation performance)  |\n",
    "|---------------|-------------|\n",
    "| Use a bigger network|    More data     |\n",
    "| Train longer | Regularization   |\n",
    "| Look for other existing NN architextures |Look for other existing NN architextures |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BlPs5Wd8GrNg"
   },
   "source": [
    "### L1 & L2 Regularlization\n",
    "- These methods of regularizaiton do so by penalizing coefficients(regression) or weights(neural networks),\n",
    "    - L1 & L2 exist in regression models as well. There, L1='Lasso Regressions' , L2='Ridge regression'\n",
    "\n",
    "- **L1 & L2 regularization add a term to the cost function.**\n",
    "\n",
    "$$Cost function = Loss (say, binary cross entropy) + Regularization term$$\n",
    "\n",
    "$$ J (w^{[1]},b^{[1]},...,w^{[L]},b^{[L]}) = \\dfrac{1}{m} \\sum^m_{i=1}\\mathcal{L}(\\hat y^{(i)}, y^{(i)})+ \\dfrac{\\lambda}{2m}\\sum^L_{l=1}||w^{[l]}||^2$$\n",
    "\n",
    "    - where $\\lambda$ is the regularization parameter. \n",
    "    - The difference between  L1 vs L2 is that L1 is just the sum of the weights whereas L2 is the sum of the _square_of the weights.  \n",
    "\n",
    "- **L1 Regularization:**\n",
    "    $$ Cost function = Loss + \\frac{\\lambda}{2m} * \\sum ||w||$$\n",
    "    - Uses the absolute value of weights and may reduce the weights down to 0. \n",
    "    \n",
    "        \n",
    "- **L2 Regularization:**:\n",
    "    $$ Cost function = Loss + \\frac{\\lambda}{2m} * \\sum ||w||^2$$\n",
    "    - Also known as weight decay, as it forces weights to decay towards zero, but never exactly 0.. \n",
    "    \n",
    "-  Regularization term $||w^{[l]}||^2 _F$  is  A.K.A. The Frobenius Norm\n",
    "    - $||w^{[l]}||^2 = \\sum^{n^{[l-1]}}_{i=1} \\sum^{n^{[l]}}_{j=1} (w_{ij}^{[l]})^2$\n",
    "\n",
    "    \n",
    "- **CHOOSING L1 OR L2:**\n",
    "    - L1 is very useful when trying to compress a model. (since weights can decreae to 0)\n",
    "    - L2 is generally preferred otherwise.\n",
    "    \n",
    "- **USING L1/L2 IN KERAS:**\n",
    "    - Add a kernel_regulaizer to a  layer.\n",
    "```python \n",
    "from keras import regularizers\n",
    "model.add(Dense(64, input_dim=64, kernel_regularizer=regularizers.l2(0.01))\n",
    "```\n",
    "    - here 0.01 = $\\lambda$\n",
    "\n",
    "### Dropout Regularization\n",
    "- Uses a specified probablity to random leave out a node from a ---epoch?\n",
    "\n",
    "\n",
    "- **USING DROPOUT IN KERAS:**\n",
    "    - Dropout layers are located in keras.layers.core \n",
    "    - Specify probably of being exlcuded/dropped out.\n",
    "```python\n",
    "from keras.layers.core import Dropout\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu'))\n",
    "model.add(layers.core.Dropout(Dropout(0.25))                              \n",
    "```\n",
    "\n",
    "### Early Stopping (not covered in class)\n",
    "- Monitor performance for decrease or plateau in performance, terminate process when given criteria is reached.\n",
    "\n",
    "- **In Keras:**\n",
    "    - Can be applied using the [callbacks function](https://keras.io/callbacks/)\n",
    "```python    \n",
    "from keras.callbacks import EarlyStopping\n",
    "EarlyStopping(monitor='val_err', patience=5)\n",
    "```\n",
    "    - 'Monitor' denotes quanitity to check\n",
    "    - 'val_err' denotes validation error\n",
    "    - 'pateience' denotes # of epochs without improvement before stopping.\n",
    "        - Be careful, as sometimes models _will_ continue to improve after a stagnant period\n",
    "\n",
    "### Reference Links I found:\n",
    "- https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/\n",
    "- http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of James' Study Group Notes - Section 41-42 (condensed version).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
